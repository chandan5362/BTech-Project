{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install utm\n",
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import utm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-growing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../Dataset/Maharashtra_Soil_Nutrients_Data.xlsx\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-twist",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['lon', 'lat', 'N']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['lon'] = 100000000*data['lon']\n",
    "# data['lat'] = 100000000*data['lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-armor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split dataset into train and test\n",
    "# split the dataset into train and test dataset\n",
    "ix = np.random.choice(data.shape[0],int(data.shape[0]*0.2),replace = False)\n",
    "\n",
    "data_train = data.iloc[[int(i) for i in range(data.shape[0]) if i not in ix]].reset_index(drop = True)\n",
    "data_test = data.iloc[ix].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-burns",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-carroll",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = (data_train-data_train.mean())/data_train.std()\n",
    "data_test = (data_test-data_test.mean())/data_test.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-measure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-sampling",
   "metadata": {},
   "source": [
    " ## NP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseNPBlock(nn.Module):\n",
    "    \"\"\"relu non-linearities for NP block\"\"\"\n",
    "    def __init__(self, inp_size,op_size, isnorm = True, bias = False, p = 0):\n",
    "        \"\"\"init function for linear2d class\n",
    "\n",
    "        parameters\n",
    "        ----------\n",
    "        inp_size : int\n",
    "                input dimension for the Encoder part (d_in)\n",
    "        op_size : int\n",
    "                output dimension for Encoder part(d_out)\n",
    "        norm : str\n",
    "                normalization to be applied on linear output\n",
    "                pass norm == 'batch' to apply batch normalization\n",
    "                else dropout normalization is applied\n",
    "        bias : bool\n",
    "                if True, bias is included for linear layer else discarded\n",
    "        p : float\n",
    "                probality to be considered while applying Dropout regularization\n",
    "                \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(inp_size,op_size,bias = bias)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.batch_norm = nn.BatchNorm2d(op_size)\n",
    "        self.dropout = nn.Dropout2d(p)\n",
    "        self.isnorm = isnorm\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        if self.isnorm:\n",
    "            x = self.batch_norm(x.permute(0,2,1)[:,:,:,None]) \n",
    "            x = self.dropout(x)\n",
    "            x = x[:,:,:,0].permute(0,2,1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_MLP(nn.Module):\n",
    "    \"\"\" Batch MLP layer for NP-Encoder\"\"\"\n",
    "    def __init__(self, in_size, op_size, num_layers, isnorm = True, p = 0):\n",
    "        \"\"\"init function for linear2d class\n",
    "        \n",
    "        parameters\n",
    "        ----------\n",
    "        inp_size : int\n",
    "                input dimension for the Encoder part (d_in)\n",
    "        op_size : int\n",
    "                output dimension for Encoder part(d_out)\n",
    "        norm : str\n",
    "                normalization to be applied on linear output\n",
    "                pass norm == 'batch' to apply batch normalization\n",
    "                else dropout normalization is applied\n",
    "                \n",
    "        return torch.tensor of size (B,num_context_points,d_out)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.op_size = op_size\n",
    "        self.num_layers = num_layers\n",
    "        self.isnorm  = isnorm\n",
    "        \n",
    "        self.first_layer = baseNPBlock(in_size, op_size, self.isnorm, False,p)\n",
    "        self.encoder = nn.Sequential(*[baseNPBlock(op_size, op_size, self.isnorm, False, p) for layer in range(self.num_layers-2)])\n",
    "        self.last_layer = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.first_layer(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.last_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self,in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_ch, out_ch, bias = False)\n",
    "#         print(\"w shape =\", self.linear.weight.shape)\n",
    "        torch.nn.init.normal_(self.linear.weight,std = in_ch**0.5) #initilize weight matrix\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         print(\"x dim =\", x.shape)\n",
    "        return self.linear(x)\n",
    "    \n",
    "    \n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim, \n",
    "        attn_type , \n",
    "        attn_layers,\n",
    "        x_dim, \n",
    "        rep='mlp',\n",
    "        n_multiheads = 8,\n",
    "        isnorm = True,\n",
    "        p = 0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.rep = rep\n",
    "\n",
    "        # rep determines whether raw input given to the model would be used as key and query or\n",
    "        # it's output through MLP. \n",
    "        if self.rep =='mlp':\n",
    "            \n",
    "            self.batch_mlpk = batch_MLP(x_dim, hidden_dim, attn_layers, isnorm ,p)\n",
    "            self.batch_mlpq = batch_MLP(x_dim, hidden_dim, attn_layers, isnorm, p)\n",
    "        \n",
    "        \n",
    "        if attn_type == 'uniform':\n",
    "            self.attn_func = self.uniform_attn\n",
    "        if attn_type=='laplace':\n",
    "            self.attn_func = self.laplace_attn\n",
    "        if attn_type == 'dot':\n",
    "            self.attn_func = self.dot_attn\n",
    "        elif attn_type == 'multihead':\n",
    "            self.w_k = nn.ModuleList([LinearAttention(hidden_dim,hidden_dim) for head in range(n_multiheads)])\n",
    "            self.w_v = nn.ModuleList([LinearAttention(hidden_dim,hidden_dim) for head in range(n_multiheads)])\n",
    "            self.w_q = nn.ModuleList([LinearAttention(hidden_dim,hidden_dim) for head in range(n_multiheads)])\n",
    "            \n",
    "            self.w = LinearAttention(hidden_dim*n_multiheads,hidden_dim)\n",
    "            self.attn_func = self.multihead_attn\n",
    "            self.num_heads = n_multiheads\n",
    "            \n",
    "            \n",
    "            \n",
    "    def forward(self, k, q, v):\n",
    "        \n",
    "        if self.rep =='mlp':\n",
    "#             print(\"in mlp\")\n",
    "            k = self.batch_mlpk(k) #(B, n, H)\n",
    "            q = self.batch_mlpq(q) #(B, m, H)\n",
    "#         print(k.shape, q.shape, v.shape)\n",
    "        rep = self.attn_func(k,q,v)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    \n",
    "    def uniform_attn(self, k, q, v):\n",
    "        num_points = q.shape[1]\n",
    "        rep = torch.mean(v, axis = 1, keepdim = True)\n",
    "        rep = rep.repeat(1,num_points,1)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    def laplace_attn(self, k, q, v, scale = 0.5):\n",
    "        k = k.unsqueeze(1)\n",
    "        v = v.unsqueeze(2)\n",
    "        \n",
    "        w = torch.abs((k-v)*scale)\n",
    "        w = w.sum(dim = -1)\n",
    "        weight = torch.softmax(w, dim = -1)\n",
    "        \n",
    "        #batch matrix multiplication (einstein summation convention for tensor)\n",
    "        rep = torch.einsum(\"bik, bkj -> bij\",weight, v)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    \n",
    "    def dot_attn(self, k, q, v):\n",
    "#         print(\"k =\",k.shape)\n",
    "#         print(\"q =\",q.shape)\n",
    "#         print(\"v =\",v.shape)    \n",
    "        β = q.shape[-1]**0.5\n",
    "        w_unnorm = torch.einsum('bjk,bik->bij', k, q)/β\n",
    "#         print(\"w_unnorm =\",w_unnorm.shape)\n",
    "        \n",
    "        weight = torch.softmax(w_unnorm, dim = -1)\n",
    "        rep = torch.einsum(\"bik, bkj -> bij\",weight, v)\n",
    "#         print(\"rep =\",rep.shape)\n",
    "        return rep\n",
    "    \n",
    "    def multihead_attn(self, k , q, v):\n",
    "        outs = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "#             print(\"k =\",k.shape)\n",
    "            k = self.w_k[i](k) #(B, n, H)\n",
    "#             print(\"k =\",k.shape)\n",
    "            q = self.w_q[i](q) #(B, m, H)\n",
    "#             print(\"q =\",q.shape)\n",
    "            v = self.w_v[i](v) #(B, n, H)\n",
    "#             print(\"v =\",v.shape)\n",
    "            out = self.dot_attn(k, q, v)\n",
    "            outs.append(out)\n",
    "            \n",
    "        outs = torch.stack(outs, dim = -1) #(B, m, H, n_heads)\n",
    "#         print(\"outs dim =\", outs.shape)\n",
    "        outs = outs.view(outs.shape[0], outs.shape[1], -1) #(B, m, n_heads*H)\n",
    "#         print(\"outs shape =\",outs.shape)\n",
    "        rep = self.w(outs) #(B, m, H)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttentionModule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicEncoder(nn.Module):\n",
    "    def __init__(\n",
    "                self,\n",
    "                in_dim,\n",
    "                x_dim,\n",
    "                isnorm = True,\n",
    "                hidden_dim = 32,\n",
    "                encoder_layer = 2,\n",
    "                rep = 'mlp',\n",
    "                self_attn_type ='dot',\n",
    "                cross_attn_type ='dot',\n",
    "                p_encoder = 0,\n",
    "                p_attention = 0,\n",
    "                attn_layers = 2,\n",
    "                use_self_attn = False\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_self_attn = use_self_attn\n",
    "        \n",
    "        self.encoder = batch_MLP(in_dim, hidden_dim, encoder_layer,isnorm, p_encoder)\n",
    "        \n",
    "        if self.use_self_attn:\n",
    "            self.self_attn = AttentionModule(hidden_dim, self_attn_type, attn_layers,x_dim, rep = 'identitiy',isnorm = isnorm, p = p_attention)\n",
    "            \n",
    "        self.cross_attn = AttentionModule(hidden_dim, cross_attn_type, attn_layers, x_dim, rep ='mlp', isnorm = isnorm, p = p_attention)\n",
    "        \n",
    "    \n",
    "    def forward(self, context_x, context_y, target_x):\n",
    "        #concatenate context_x, context_y along the last dim.\n",
    "        det_enc_in = torch.cat([context_x, context_y], dim = -1)\n",
    "        \n",
    "        det_encoded = self.encoder(det_enc_in) #(B, n, hd)\n",
    "#         print(\"in det 1\",det_encoded.shape)\n",
    "        if self.use_self_attn:\n",
    "            det_encoded = self.self_attn(det_encoded, det_encoded, det_encoded) #(B, n, hd)\n",
    "#         print(\"in det 2\",det_encoded.shape)    \n",
    "        h = self.cross_attn(context_x, target_x, det_encoded) #(B, n, hd)\n",
    "        \n",
    "        return h\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_dim,\n",
    "                 x_dim,\n",
    "                hidden_dim = 32,\n",
    "                latent_dim = 32,\n",
    "                self_attn_type = 'dot',\n",
    "                encoder_layer = 3,\n",
    "                min_std = 0.01,\n",
    "                isnorm = True,\n",
    "                p_encoder = 0,\n",
    "                p_attn = 0,\n",
    "                use_self_attn = False,\n",
    "                attn_layers = 2,\n",
    "                 rep ='mlp'\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self._use_attn = use_self_attn\n",
    "        \n",
    "        self.encoder = batch_MLP(in_dim, hidden_dim, encoder_layer,isnorm, p_encoder)\n",
    "        \n",
    "        if self._use_attn:\n",
    "            self.self_attn = AttentionModule(hidden_dim, self_attn_type, attn_layers,x_dim, rep = rep,isnorm = isnorm, p = p_attn)\n",
    "        \n",
    "        self.secondlast_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.l_sigma = nn.Linear(hidden_dim, latent_dim) \n",
    "        self.min_std = min_std\n",
    "#         self.use_lvar = use_lvar\n",
    "        self.use_attn = use_self_attn\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x,y):\n",
    "        encoder_inp = torch.cat([x,y], dim = -1) \n",
    "        \n",
    "        encoded_op = self.encoder(encoder_inp)#(B, n, hd)\n",
    "#         print(\"encoder_op shape = \",encoded_op.shape)\n",
    "        print(encoded_op)\n",
    "        if self.use_attn:\n",
    "            encoded_op = self.self_attn(encoded_op, encoded_op, encoded_op) #(B, n, hd)\n",
    "            \n",
    "        print(encoded_op.shape)\n",
    "        mean_val = torch.mean(encoded_op, dim = 1) #mean aggregation (B, hd)\n",
    "        \n",
    "        #further MLP layer that maps parameters to gaussian latent\n",
    "        mean_repr = torch.relu(self.secondlast_layer(mean_val)) #(B, hd)\n",
    "        \n",
    "        μ = self.mean(mean_repr) # (B, ld)\n",
    "#         print(\"mean = \", μ.shape)\n",
    "        log_scale = self.l_sigma(mean_repr) #(B, ld)\n",
    "        \n",
    "        #to avoid mode collapse\n",
    "        σ = self.min_std + (1-self.min_std)*torch.sigmoid(log_scale*0.5) #(B, ld)\n",
    "#         print(μ)\n",
    "#         print(μ.shape)\n",
    "        dist = torch.distributions.Normal(μ, σ)\n",
    "        \n",
    "        return dist\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 x_dim,\n",
    "                 y_dim,\n",
    "                 hidden_dim = 32,\n",
    "                 latent_dim = 32,\n",
    "                 n_decoder_layer = 3,\n",
    "                 use_deterministic_path = True,\n",
    "                 min_std = 0.01,\n",
    "                 isnorm = True,\n",
    "                 dropout_p = 0,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.isnorm = isnorm\n",
    "        self.target_transform = nn.Linear(x_dim, hidden_dim)\n",
    "        \n",
    "        if use_deterministic_path:\n",
    "            hidden_dim_2 = 2 * hidden_dim + latent_dim\n",
    "        else:\n",
    "            hidden_dim_2 = hidden_dim + latent_dim\n",
    "            \n",
    "        self.decoder = batch_MLP(hidden_dim_2, hidden_dim_2, n_decoder_layer, isnorm, dropout_p)\n",
    "        \n",
    "        self.mean = nn.Linear(hidden_dim_2, y_dim)\n",
    "        self.std = nn.Linear(hidden_dim_2, y_dim)\n",
    "        self.deterministic_path = use_deterministic_path\n",
    "        self.min_std = min_std\n",
    "        \n",
    "        \n",
    "    def forward(self, r, z, t_x):\n",
    "        x = self.target_transform(t_x)\n",
    "        \n",
    "        if self.deterministic_path:\n",
    "            z = torch.cat([r,z], dim = -1)\n",
    "#             print(\"z.shape =\", z.shape)\n",
    "        r = torch.cat([z,x], dim = -1)\n",
    "        \n",
    "        r = self.decoder(r)\n",
    "        \n",
    "        mean = self.mean(r)\n",
    "        log_sigma = self.std(r)\n",
    "        \n",
    "        #clamp sigmad\n",
    "        sigma = self.min_std + (1 - self.min_std) * F.softplus(log_sigma)\n",
    "        \n",
    "        dist = torch.distributions.Normal(mean,sigma)\n",
    "        \n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentModel(nn.Module):\n",
    "    def __init__(self,\n",
    "               x_dim,\n",
    "               y_dim,\n",
    "               hidden_dim = 32,\n",
    "               latent_dim = 32,\n",
    "               latent_self_attn_type = 'multihead',\n",
    "               det_self_attn_type = 'multihead',\n",
    "               det_cross_attn_type = 'multihead',\n",
    "               n_lat_enc_layer = 2,\n",
    "               n_det_enc_layer = 2,\n",
    "               n_decoder_layer = 2,\n",
    "               rep ='mlp',\n",
    "               use_deterministic_enc = False,\n",
    "               min_std = 0.01,\n",
    "               p_drop = 0,\n",
    "               isnorm = True,\n",
    "               p_attn_drop = 0,\n",
    "               attn_layers = 2,\n",
    "               use_self_attn = False,\n",
    "               context_in_target = True\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.laten_encoder = LatentEncoder(x_dim+y_dim,\n",
    "                                           x_dim,\n",
    "                                           hidden_dim=hidden_dim,\n",
    "                                           latent_dim=latent_dim,\n",
    "                                           self_attn_type=latent_self_attn_type,\n",
    "                                           encoder_layer=n_lat_enc_layer,\n",
    "                                           min_std=min_std,\n",
    "                                           isnorm = isnorm,\n",
    "                                           p_encoder=p_drop,\n",
    "                                           p_attn=p_attn_drop,\n",
    "                                           rep = 'identity',\n",
    "                                           use_self_attn=use_self_attn,\n",
    "                                           attn_layers=attn_layers \n",
    "                                          )\n",
    "        self.deterministic_encoder = DeterministicEncoder(x_dim+y_dim,\n",
    "                                                          x_dim,\n",
    "                                                          isnorm = isnorm,\n",
    "                                                          hidden_dim=hidden_dim,\n",
    "                                                          encoder_layer=n_det_enc_layer,\n",
    "                                                          rep=rep,\n",
    "                                                          self_attn_type=det_self_attn_type,\n",
    "                                                          cross_attn_type=det_cross_attn_type,\n",
    "                                                          p_encoder=p_drop,\n",
    "                                                          p_attention=p_attn_drop,\n",
    "                                                          attn_layers=attn_layers,\n",
    "                                                          use_self_attn=use_self_attn\n",
    "                                                         )\n",
    "        self.decoder = Decoder(x_dim,\n",
    "                              y_dim,\n",
    "                              hidden_dim  = hidden_dim,\n",
    "                              latent_dim=latent_dim,\n",
    "                              n_decoder_layer=n_decoder_layer,\n",
    "                              use_deterministic_path=use_deterministic_enc,\n",
    "                              min_std=min_std,\n",
    "                              isnorm=isnorm,\n",
    "                              dropout_p=p_drop\n",
    "                              )\n",
    "        self.use_deterministic_enc = use_deterministic_enc\n",
    "        self.context_in_target = context_in_target\n",
    "        \n",
    "        \n",
    "    def forward(self, c_x, c_y, t_x, t_y = None, training = False):\n",
    "        dist_prior = self.laten_encoder(c_x, c_y)\n",
    "\n",
    "        if t_y is not None:\n",
    "            dist_posterior = self.laten_encoder(t_x, t_y)\n",
    "            z = dist_posterior.loc\n",
    "        else:\n",
    "            z = dist_prior.loc\n",
    "            \n",
    "        n_target = t_x.shape[1]\n",
    "        z = z.unsqueeze(1).repeat(1, n_target,1) #(B, n_target, L)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.use_deterministic_enc:\n",
    "            r = self.deterministic_encoder(c_x, c_y, t_x) #(B, n_target=m, H)\n",
    "#             print(r.shape)\n",
    "        else:\n",
    "            r = None\n",
    "            \n",
    "        dist = self.decoder(r, z, t_x)\n",
    "        \n",
    "        #at test time, target y is not Known so we return None\n",
    "        if t_y is not None:\n",
    "            log_p = dist.log_prob(t_y).mean(-1)\n",
    "            kl_loss = torch.distributions.kl_divergence(dist_posterior, dist_prior).mean(-1)\n",
    "            kl_loss = kl_loss[:,None].expand(log_p.shape)\n",
    "            loss = (kl_loss-log_p).mean()\n",
    "            mse_loss = F.mse_loss(dist.loc, t_y, reduction = 'none')[:,:c_x.size(1)].mean()\n",
    "        else:\n",
    "            kl_loss = None\n",
    "            log_p = None\n",
    "            mse_loss = None\n",
    "            loss = None\n",
    "            \n",
    "        y_pred = dist.rsample() if self.training else dist.loc\n",
    "            \n",
    "        return y_pred,  dict(loss = loss, loss_p = log_p, loss_kl = kl_loss, loss_mse = mse_loss), dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-westminster",
   "metadata": {},
   "source": [
    "## Data loading in torch.Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NutrientsDataset(Dataset):\n",
    "    def __init__(self, df, num_context=40, num_extra_target=10):\n",
    "        self.df = df\n",
    "        self.num_context = num_context\n",
    "        self.num_extra_target = num_extra_target\n",
    "\n",
    "    def get_rows(self, i):\n",
    "        rows = self.df.iloc[i : i + (self.num_context + self.num_extra_target)].copy()\n",
    "        x = rows.iloc[:,:2].copy()\n",
    "        y = rows.iloc[:,2:].copy()\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.get_rows(i)\n",
    "        return x.values, y.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df) - (self.num_context + self.num_extra_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npsample_batch(x, y, size=None, sort=False):\n",
    "    \n",
    "    \"\"\"Sample from numpy arrays along 2nd dim.\"\"\"\n",
    "    inds = np.random.choice(range(x.shape[1]), size=size, replace=False)\n",
    "    return x[:, inds], y[:, inds]\n",
    "\n",
    "def collate_fns(max_num_context, max_num_extra_target, sample, sort=True, context_in_target=True):\n",
    "    def collate_fn(batch, sample=sample):\n",
    "        x = np.stack([x for x, y in batch], 0)\n",
    "        y = np.stack([y for x, y in batch], 0)\n",
    "\n",
    "        # Sample a subset of random size\n",
    "        num_context = np.random.randint(4, max_num_context)\n",
    "        num_extra_target = np.random.randint(4, max_num_extra_target)\n",
    "\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "\n",
    "        \n",
    "        x_context = x[:, :max_num_context]\n",
    "        y_context = y[:, :max_num_context]\n",
    "    \n",
    "        x_target_extra = x[:, max_num_context:]\n",
    "        y_target_extra = y[:, max_num_context:]\n",
    "        \n",
    "        if sample:\n",
    "\n",
    "            x_context, y_context = npsample_batch(\n",
    "                x_context, y_context, size=num_context\n",
    "            )\n",
    "\n",
    "            x_target_extra, y_target_extra = npsample_batch(\n",
    "                x_target_extra, y_target_extra, size=num_extra_target, sort=sort\n",
    "            )\n",
    "\n",
    "        # do we want to compute loss over context+target_extra, or focus in on only target_extra?\n",
    "        if context_in_target:\n",
    "            x_target = torch.cat([x_context, x_target_extra], 1)\n",
    "            y_target = torch.cat([y_context, y_target_extra], 1)\n",
    "        else:\n",
    "            x_target = x_target_extra\n",
    "            y_target = y_target_extra\n",
    "\n",
    "        \n",
    "        return x_context, y_context, x_target, y_target\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data loader\n",
    "hparams = dict(num_context = 40,\n",
    "               num_extra_target = 16,\n",
    "               batch_size = 16,\n",
    "               context_in_target = False)\n",
    "train_df = NutrientsDataset(data_train,hparams['num_context'],hparams['num_extra_target'])\n",
    "\n",
    "train_loader = DataLoader(train_df,\n",
    "                          batch_size=hparams['batch_size'],\n",
    "                         shuffle = True,\n",
    "                         collate_fn=collate_fns(\n",
    "                             hparams['num_context'],hparams['num_extra_target'], True, hparams['context_in_target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-franklin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#eval loss\n",
    "def validation(data_train, data_test, do_eval=True):\n",
    "    \"\"\"Run model on test/val data\"\"\"\n",
    "    if do_eval:\n",
    "        Regressor.eval()\n",
    "    with torch.no_grad():\n",
    "        target_x, target_y = data_test.iloc[:,:2], data_test.iloc[:,2:]\n",
    "        context_x, context_y = data_train.iloc[:,:2], data_train.iloc[:,2:]\n",
    "#         print(context_x.shape, context_y.shape)\n",
    "\n",
    "        context_x = torch.from_numpy(context_x.values).float()[None, :].to(device)\n",
    "        context_y = torch.from_numpy(context_y.values).float()[None, :].to(device)\n",
    "        target_x = torch.from_numpy(target_x.values).float()[None, :].to(device)\n",
    "        target_y = torch.from_numpy(target_y.values).float()[None, :].to(device)\n",
    "#         print(context_x.shape, context_y.shape, target_x.shape, target_y.shape)\n",
    "        y_pred, losses, extra = Regressor.forward(context_x, context_y, target_x, target_y, training = False)\n",
    "#         print(y_pred.shape)\n",
    "    yr=(target_y-y_pred)[0].detach().cpu().numpy()\n",
    "#     print(yr)\n",
    "    return yr, y_pred, losses, extra "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-philadelphia",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regressor = LatentModel(2,1,\n",
    "                       p_drop = 0.3,\n",
    "                        p_attn_drop=0.3,\n",
    "                        hidden_dim = 32,\n",
    "                        latent_dim = 64,\n",
    "                       n_decoder_layer = 6,\n",
    "                        n_lat_enc_layer=4,\n",
    "                       isnorm = True,\n",
    "#                         use_self_attn=True,\n",
    "                        use_deterministic_enc=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-petersburg",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-rotation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(Regressor.parameters(), lr = 1e-4, weight_decay=0.01)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(opt,\n",
    "#                                                step_size=5,\n",
    "#                                                gamma=0.1)\n",
    "Regressor = Regressor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-married",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "val_norm = data_test.shape[0]\n",
    "mse_loss_train = []\n",
    "mse_loss_eval = []\n",
    "elbo_loss_train  = []\n",
    "elbo_loss_eval = []\n",
    "mae_val_loss = []\n",
    "for epoch in range(10):\n",
    "    loss = 0 \n",
    "    mse_loss = 0\n",
    "    Regressor.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        context_x, context_y, target_x, target_y = batch\n",
    "        cx =context_x.to(device)\n",
    "        cy = context_y.to(device)\n",
    "        tx = target_x.to(device)\n",
    "        ty = target_y.to(device)\n",
    "        Regressor.zero_grad()\n",
    "        y_pred, losses, extra = Regressor.forward(cx, cy, tx, ty, training=True)\n",
    "        losses['loss'].backward()\n",
    "        loss += losses['loss'].cpu().detach().numpy()\n",
    "        mse_loss+=losses['loss_mse'].cpu().detach().numpy()\n",
    "        opt.step()\n",
    "        \n",
    "    loss /= len(train_loader)\n",
    "    elbo_loss_train.append(loss)\n",
    "    mse_loss_train.append(mse_loss/len(train_loader))\n",
    "    print(epoch)\n",
    "    print('ELBO train_loss', loss)\n",
    "    print('mse train_loss', mse_loss/len(train_loader))\n",
    "    \n",
    "    yr, ypred, losses_val, extra = validation(data_train, data_test)\n",
    "    mse_loss_val = losses_val['loss_mse'].cpu().detach().numpy()\n",
    "    mse_loss_eval.append(mse_loss_val)\n",
    "    elbo_loss_val = losses_val['loss'].cpu().detach().numpy() \n",
    "    val_loss = np.mean(np.abs(yr))\n",
    "    elbo_loss_eval.append(elbo_loss_val)\n",
    "    mae_val_loss.append(val_loss)\n",
    "    \n",
    "#     lr_scheduler.step()\n",
    "    print('ELBO val_loss', elbo_loss_val)\n",
    "    print('mse val_loss', mse_loss_val)\n",
    "    print('mae val_loss', val_loss)\n",
    "    print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-robinson",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-accident",
   "metadata": {},
   "source": [
    "### Train vs val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x  = np.arange(0,234,1)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(train_loss, val_loss):\n",
    "#     x = x  = np.arange(0,100,1)\n",
    "    plt.plot(train_loss, label =\"train\")\n",
    "    plt.plot(val_loss, label = \"val\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbo loss plot\n",
    "loss_plot(elbo_loss_train, elbo_loss_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse loss plot\n",
    "loss_plot(mse_loss_train, mse_loss_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RMSE for trainig and testing error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "yr, y_pred, losses, extra = validation(data_train, data_train)\n",
    "\n",
    "y_pred = y_pred[0].detach().cpu().numpy().flatten()\n",
    "\n",
    "MAE = mean_absolute_error(data_train.iloc[:,2], y_pred)\n",
    "RMSE = mean_squared_error(data_train.iloc[:,2], y_pred)\n",
    "\n",
    "print(\"MAE = \", MAE)\n",
    "print(\"RMSE = \", RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.iloc[:,2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-request",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "productive-poetry",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid():\n",
    "    lon = np.linspace(17.895,20.598,10000)\n",
    "    lat = np.linspace(73.384,76.675,10000)\n",
    "    return lon,lat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon, lat = create_grid()\n",
    "xt = np.asarray((lon, lat)).T\n",
    "xt = (xt-np.mean(xt))/np.std(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(do_eval=True):\n",
    "    \"\"\"Run model on test/val data\"\"\"\n",
    "    if do_eval:\n",
    "        Regressor.eval()\n",
    "    with torch.no_grad():\n",
    "        target_x = xt\n",
    "        context_x, context_y = data_train.iloc[:,:2], data_train.iloc[:,2:]\n",
    "#         print(target_x)\n",
    "\n",
    "        context_x = torch.from_numpy(context_x.values).float()[None, :].to(device)\n",
    "        context_y = torch.from_numpy(context_y.values).float()[None, :].to(device)\n",
    "        target_x = torch.from_numpy(target_x).float()[None, :].to(device)\n",
    "\n",
    "        y_pred, losses, extra = Regressor.forward(context_x, context_y, target_x, training = False)\n",
    "#         print(y_pred.shape)\n",
    "#     yr=(target_y-y_pred)[0].detach().cpu().numpy()\n",
    "#     print(yr)\n",
    "    return  y_pred, losses, extra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred, _, _ = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-trinidad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ypred.detach().cpu().numpy().reshape((100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "from osgeo import gdal_array\n",
    "from osgeo import osr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.transform import Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-calibration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lon =lon.reshape(100,100)\n",
    "lat =lat.reshape(100,100)\n",
    "# z = np.reshape()\n",
    "xmin,ymin,xmax,ymax = [lon.min(),lat.min(),lon.max(),lat.max()]\n",
    "nrows,ncols = z.shape\n",
    "xres = (xmax-xmin)/float(ncols)\n",
    "yres = (ymax-ymin)/float(nrows)\n",
    "geotransform=(xmin,xres,0,ymax,0, -yres)\n",
    "output_raster = gdal.GetDriverByName('GTiff').Create('myraster.tif',ncols, nrows, 1 ,gdal.GDT_Float32)\n",
    "output_raster.SetGeoTransform(geotransform)\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromEPSG(4326)\n",
    "output_raster.SetProjection( srs.ExportToWkt() )\n",
    "output_raster.GetRasterBand(1).WriteArray(z)\n",
    "output_raster.FlushCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lon, lat = create_grid()\n",
    "# res = (lon[-1] - lon[0]) / 10\n",
    "# transform = Affine.translation(lon[0] - res / 2, lat[0] - res / 2) * Affine.scale(res, res)\n",
    "# # transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx,yy = np.meshgrid(lon, lat)\n",
    "# ar = []\n",
    "# for i in range(xx.shape[0]):\n",
    "#     for j in range(yy.shape[1]):\n",
    "#         ar.append([xx[i,j],yy[i,j]])\n",
    "        \n",
    "# x_t = np.asarray(ar)\n",
    "# x_t = (x_t-np.mean(x_t))/np.std(x_t)\n",
    "# # x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-ghost",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-separate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset = rasterio.open('./myraster.tif',  'w',\n",
    "#          driver='GTiff',\n",
    "#         height=Z.shape[0],\n",
    "#          width=Z.shape[1],\n",
    "#          count=1,\n",
    "#          dtype=Z.dtype,\n",
    "#         crs='+proj=latlong',\n",
    "#          transform=transform,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset.write(Z, 1)\n",
    "# new_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.plot import show\n",
    "src = rasterio.open(\"./\")\n",
    "show(src.read(), transform=src.transform)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-export",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-surprise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To define function to find batch size for training the model\n",
    "# use this function to find out the batch size\n",
    "#https://stackoverflow.com/questions/46654424/how-to-calculate-optimal-batch-size\n",
    "def FindBatchSize(model):\n",
    "    \"\"\"#model: model architecture, that is yet to be trained\"\"\"\n",
    "    import os, sys, psutil, gc, tensorflow, keras\n",
    "    import numpy as np\n",
    "    from keras import backend as K\n",
    "    BatchFound= 16\n",
    "\n",
    "    try:\n",
    "        total_params= int(model.count_params());    GCPU= \"CPU\"\n",
    "        #find whether gpu is available\n",
    "        try:\n",
    "            if K.tensorflow_backend._get_available_gpus()== []:\n",
    "                GCPU= \"CPU\";    #CPU and Cuda9GPU\n",
    "            else:\n",
    "                GCPU= \"GPU\"\n",
    "        except:\n",
    "            from tensorflow.python.client import device_lib;    #Cuda8GPU\n",
    "            def get_available_gpus():\n",
    "                local_device_protos= device_lib.list_local_devices()\n",
    "                return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "            if \"gpu\" not in str(get_available_gpus()).lower():\n",
    "                GCPU= \"CPU\"\n",
    "            else:\n",
    "                GCPU= \"GPU\"\n",
    "\n",
    "        #decide batch size on the basis of GPU availability and model complexity\n",
    "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params <1000000):\n",
    "            BatchFound= 64    \n",
    "        if (os.cpu_count() <16) and (total_params <500000):\n",
    "            BatchFound= 64  \n",
    "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params <2000000) and (total_params >=1000000):\n",
    "            BatchFound= 32      \n",
    "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params >=2000000) and (total_params <10000000):\n",
    "            BatchFound= 16  \n",
    "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params >=10000000):\n",
    "            BatchFound= 8       \n",
    "        if (os.cpu_count() <16) and (total_params >5000000):\n",
    "            BatchFound= 8    \n",
    "        if total_params >100000000:\n",
    "            BatchFound= 1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "\n",
    "        #find percentage of memory used\n",
    "        memoryused= psutil.virtual_memory()\n",
    "        memoryused= float(str(memoryused).replace(\" \", \"\").split(\"percent=\")[1].split(\",\")[0])\n",
    "        if memoryused >75.0:\n",
    "            BatchFound= 8\n",
    "        if memoryused >85.0:\n",
    "            BatchFound= 4\n",
    "        if memoryused >90.0:\n",
    "            BatchFound= 2\n",
    "        if total_params >100000000:\n",
    "            BatchFound= 1\n",
    "        print(\"Batch Size:  \"+ str(BatchFound));    gc.collect()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    memoryused= [];    total_params= [];    GCPU= \"\";\n",
    "    del memoryused, total_params, GCPU;    gc.collect()\n",
    "    return BatchFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "FindBatchSize(Regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-class",
   "metadata": {},
   "source": [
    "# Meuse data testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,requests\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "allow_columns = ['lead', 'elev', 'dist', 'x', 'y', 'ffreq', 'soil']\n",
    "label ='lead'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "meuse = gpd.read_file('meuse')\n",
    "meuse.crs = {'init':'epsg:28992'}\n",
    "meuse['x'] = meuse['geometry'].apply(lambda x: x.x)\n",
    "meuse['y'] = meuse['geometry'].apply(lambda x: x.y)\n",
    "meuse.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['ffreq', 'soil']:\n",
    "    meuse[col] = pd.to_numeric(meuse[col]) * 1.0\n",
    "meuse[allow_columns].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_indexes = np.random.choice(a=meuse.index, size=int(np.round(len(meuse.index.values)/4)))\n",
    "train_indexes = [index for index in meuse.index if index not in test_indexes]\n",
    "meuse_test = meuse.loc[test_indexes,:].copy()\n",
    "meuse_train = meuse.loc[train_indexes,:].copy()\n",
    "print('Number of observations in training: {}, in test: {}'.format(len(meuse_train), len(meuse_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = meuse_train[allow_columns].copy()\n",
    "norm_mean = meuse[allow_columns].mean()\n",
    "norm_std = meuse[allow_columns].std()\n",
    "\n",
    "hparams = dict(\n",
    "    num_context=15,\n",
    "    num_extra_target=16,\n",
    "    batch_size=40,\n",
    "    context_in_target=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, num_context=40, num_extra_target=10, label_names=['lead']):\n",
    "        self.df = df\n",
    "        self.num_context = num_context\n",
    "        self.num_extra_target = num_extra_target\n",
    "        self.label_names = label_names\n",
    "\n",
    "    def get_rows(self, i):\n",
    "        rows = self.df.iloc[i : i + (self.num_context + self.num_extra_target)].copy()\n",
    "\n",
    "        # make sure tstp, which is our x axis, is the first value\n",
    "\n",
    "        # This will be the last row, and will change it upon sample to let the model know some points are in the future\n",
    "\n",
    "        x = rows.drop(columns=self.label_names).copy()\n",
    "        y = rows[self.label_names].copy()\n",
    "#         print(x.shape)\n",
    "#         print(y.shape)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.get_rows(i)\n",
    "        return x.values, y.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df) - (self.num_context + self.num_extra_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(loader_train):\n",
    "#     a,b,c,d = batch\n",
    "#     print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = meuse_train[allow_columns].copy()\n",
    "df_train -= norm_mean\n",
    "df_train /= norm_std\n",
    "\n",
    "data_train = DataSet(\n",
    "    df_train, hparams[\"num_context\"], hparams[\"num_extra_target\"]\n",
    ")\n",
    "loader_train = torch.utils.data.DataLoader(\n",
    "    data_train,\n",
    "    batch_size=hparams[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fns(\n",
    "        hparams[\"num_context\"], hparams[\"num_extra_target\"], sample=True, context_in_target=hparams[\"context_in_target\"]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = meuse_test[allow_columns].copy()\n",
    "df_test -= norm_mean\n",
    "df_test /= norm_std\n",
    "\n",
    "data_test = DataSet(\n",
    "    df_test, hparams[\"num_context\"], hparams[\"num_extra_target\"]\n",
    ")\n",
    "loader_test = torch.utils.data.DataLoader(\n",
    "    data_test,\n",
    "    batch_size=hparams[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fns(\n",
    "        hparams[\"num_context\"], hparams[\"num_extra_target\"], sample=False, context_in_target=hparams[\"context_in_target\"]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LatentModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regressor = LatentModel(len(allow_columns)-1,1,\n",
    "                       p_drop = 0.5,\n",
    "                        hidden_dim = 64,\n",
    "                        latent_dim = 16,\n",
    "                        n_lat_enc_layer=3,\n",
    "                        n_det_enc_layer=3,\n",
    "                       n_decoder_layer = 3,\n",
    "                       isnorm = True,\n",
    "                       context_in_target=False,\n",
    "                        use_self_attn=True,\n",
    "                        use_deterministic_enc = True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(Regressor.parameters(), lr=3e-4, weight_decay = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "def test(do_eval=True):\n",
    "    \"\"\"Run model on test/val data\"\"\"\n",
    "    if do_eval:\n",
    "        Regressor.eval()\n",
    "    with torch.no_grad():\n",
    "        target_x, target_y = df_test.drop(columns=['lead']), df_test['lead']\n",
    "        context_x, context_y = df_train.drop(columns=['lead']), df_train['lead']\n",
    "\n",
    "        context_x = torch.from_numpy(context_x.values).float()[None, :]\n",
    "        context_y = torch.from_numpy(context_y.values).float()[None, :, None]\n",
    "        target_x = torch.from_numpy(target_x.values).float()[None, :]\n",
    "        target_y = torch.from_numpy(target_y.values).float()[None, :, None]\n",
    "\n",
    "        y_pred, losses, extra = Regressor.forward(context_x, context_y, target_x, target_y, training= False)\n",
    "    yr=(target_y-y_pred)[0].detach().cpu().numpy() * norm_std['lead']\n",
    "    \n",
    "    return yr, y_pred, losses, extra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-flush",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss = 0 \n",
    "    Regressor.train()\n",
    "    for batch in tqdm(loader_train):\n",
    "        context_x, context_y, target_x, target_y = batch\n",
    "        print(context_x.shape)\n",
    "        Regressor.zero_grad()\n",
    "        y_pred, losses, extra = Regressor.forward(context_x, context_y, target_x, target_y, training=True)\n",
    "\n",
    "        losses['loss'].backward()\n",
    "        loss += losses['loss'].cpu().detach().numpy()\n",
    "        opt.step()\n",
    "    loss /= len(loader_train)\n",
    "    \n",
    "    print(epoch)\n",
    "    print('train_loss', loss)\n",
    "    val_loss = test()[0]\n",
    "    val_loss = np.mean(np.abs(val_loss))\n",
    "    print('val_loss', val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-culture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sublime-african",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-pocket",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
