{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "graduate-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install utm\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "equipped-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import utm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "official-growing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>OC</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.401111</td>\n",
       "      <td>17.894722</td>\n",
       "      <td>1.08</td>\n",
       "      <td>756.0</td>\n",
       "      <td>9.43</td>\n",
       "      <td>834.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73.401389</td>\n",
       "      <td>17.894722</td>\n",
       "      <td>1.12</td>\n",
       "      <td>781.2</td>\n",
       "      <td>9.21</td>\n",
       "      <td>265.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73.402222</td>\n",
       "      <td>17.894722</td>\n",
       "      <td>0.68</td>\n",
       "      <td>478.8</td>\n",
       "      <td>8.99</td>\n",
       "      <td>318.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.403056</td>\n",
       "      <td>17.894722</td>\n",
       "      <td>1.76</td>\n",
       "      <td>1234.8</td>\n",
       "      <td>9.65</td>\n",
       "      <td>954.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.403333</td>\n",
       "      <td>17.894722</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1247.4</td>\n",
       "      <td>8.77</td>\n",
       "      <td>371.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lon        lat    OC       N     P       K\n",
       "0  73.401111  17.894722  1.08   756.0  9.43  834.37\n",
       "1  73.401389  17.894722  1.12   781.2  9.21  265.10\n",
       "2  73.402222  17.894722  0.68   478.8  8.99  318.96\n",
       "3  73.403056  17.894722  1.76  1234.8  9.65  954.77\n",
       "4  73.403333  17.894722  1.78  1247.4  8.77  371.77"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"../Dataset/Maharashtra_Soil_Nutrients_Data.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-lancaster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incorporate-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_coord(x,y):\n",
    "    \"\"\"\n",
    "    parameters\n",
    "    ----------\n",
    "    x : numpy array, float64\n",
    "        list of longitude cordinates\n",
    "    y : numpy array, float64\n",
    "        list of latitude cordinates\n",
    "        \n",
    "    return\n",
    "    ------\n",
    "    scaled(0-1) x and y\n",
    "    \"\"\"\n",
    "    x = (x-x.min())/(x.max()-x.min())\n",
    "    y = (y-y.min())/(y.max()-y.min())\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "contained-friday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "val_col = ['OC','N','P','K']\n",
    "values = data[val_col]\n",
    "coordinates = data[['lon','lat']]\n",
    "#lat,lon to utm projection\n",
    "\n",
    "x,y,zone,ut = utm.from_latlon(coordinates['lat'].values,coordinates['lon'].values)\n",
    "\n",
    "lon,lat = y/1000,x/1000 #in km\n",
    "\n",
    "# lon, lat = scaled_coord(lon,lat)\n",
    "#normalize values of OC, N, K, P\n",
    "\n",
    "#standardise lon and lat\n",
    "lon = (lon-np.mean(lon))/np.std(lon)\n",
    "lat = (lat-np.mean(lat))/np.std(lat)\n",
    "\n",
    "test_k = MinMaxScaler().fit_transform(values)\n",
    "values = test_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "about-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lon'] = lon\n",
    "data['lat'] = lat\n",
    "for i,col in enumerate(val_col):\n",
    "    data[col] = values[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "headed-twist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>OC</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.425536</td>\n",
       "      <td>-2.402803</td>\n",
       "      <td>0.011632</td>\n",
       "      <td>0.079661</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.012262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.425540</td>\n",
       "      <td>-2.402303</td>\n",
       "      <td>0.012067</td>\n",
       "      <td>0.082316</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.003893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.425550</td>\n",
       "      <td>-2.400805</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.050450</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.004684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.425561</td>\n",
       "      <td>-2.399307</td>\n",
       "      <td>0.019024</td>\n",
       "      <td>0.130115</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.014032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.425565</td>\n",
       "      <td>-2.398808</td>\n",
       "      <td>0.019241</td>\n",
       "      <td>0.131443</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.005461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lon       lat        OC         N         P         K\n",
       "0 -1.425536 -2.402803  0.011632  0.079661  0.001163  0.012262\n",
       "1 -1.425540 -2.402303  0.012067  0.082316  0.001136  0.003893\n",
       "2 -1.425550 -2.400805  0.007283  0.050450  0.001109  0.004684\n",
       "3 -1.425561 -2.399307  0.019024  0.130115  0.001190  0.014032\n",
       "4 -1.425565 -2.398808  0.019241  0.131443  0.001082  0.005461"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "billion-armor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split dataset into train and test\n",
    "# split the dataset into train and test dataset\n",
    "ix = np.random.choice(data.shape[0],int(data.shape[0]*0.2),replace = False)\n",
    "data_train = data.iloc[[int(i) for i in range(data.shape[0]) if i not in ix]].reset_index(drop = True)\n",
    "data_test = data.iloc[ix].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "assisted-burns",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20837, 6), (5209, 6))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "revolutionary-milwaukee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>OC</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.425536</td>\n",
       "      <td>-2.402803</td>\n",
       "      <td>0.011632</td>\n",
       "      <td>0.079661</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.012262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.425540</td>\n",
       "      <td>-2.402303</td>\n",
       "      <td>0.012067</td>\n",
       "      <td>0.082316</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.003893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.425550</td>\n",
       "      <td>-2.400805</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.050450</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.004684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.425565</td>\n",
       "      <td>-2.398808</td>\n",
       "      <td>0.019241</td>\n",
       "      <td>0.131443</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.005461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.425572</td>\n",
       "      <td>-2.397809</td>\n",
       "      <td>0.015110</td>\n",
       "      <td>0.103560</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.010119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lon       lat        OC         N         P         K\n",
       "0 -1.425536 -2.402803  0.011632  0.079661  0.001163  0.012262\n",
       "1 -1.425540 -2.402303  0.012067  0.082316  0.001136  0.003893\n",
       "2 -1.425550 -2.400805  0.007283  0.050450  0.001109  0.004684\n",
       "3 -1.425565 -2.398808  0.019241  0.131443  0.001082  0.005461\n",
       "4 -1.425572 -2.397809  0.015110  0.103560  0.001109  0.010119"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "minute-consultation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>OC</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.168765</td>\n",
       "      <td>0.293795</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>0.055319</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.008052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.164527</td>\n",
       "      <td>-0.516612</td>\n",
       "      <td>0.009784</td>\n",
       "      <td>0.018331</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.005067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.273688</td>\n",
       "      <td>-0.876234</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>0.039828</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.009052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.217589</td>\n",
       "      <td>0.415471</td>\n",
       "      <td>0.009675</td>\n",
       "      <td>0.066383</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.004435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.596328</td>\n",
       "      <td>-0.386163</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>0.019004</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.006944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lon       lat        OC         N         P         K\n",
       "0 -1.168765  0.293795  0.008044  0.055319  0.001278  0.008052\n",
       "1 -1.164527 -0.516612  0.009784  0.018331  0.001343  0.005067\n",
       "2 -1.273688 -0.876234  0.005761  0.039828  0.001752  0.009052\n",
       "3 -1.217589  0.415471  0.009675  0.066383  0.001641  0.004435\n",
       "4 -0.596328 -0.386163  0.007718  0.019004  0.000443  0.006944"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-shepherd",
   "metadata": {},
   "source": [
    "## GP Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vital-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "small-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The (A)NP takes as input a `NPRegressionDescription` namedtuple with fields:\n",
    "#   `query`: a tuple containing ((context_x, context_y), target_x)\n",
    "#   `target_y`: a tensor containing the ground truth for the targets to be\n",
    "#     predicted\n",
    "#   `num_total_points`: A vector containing a scalar that describes the total\n",
    "#     number of datapoints used (context + target)\n",
    "#   `num_context_points`: A vector containing a scalar that describes the number\n",
    "#     of datapoints used as context\n",
    "# The GPCurvesReader returns the newly sampled data in this format at each\n",
    "# iteration\n",
    "\n",
    "NPRegressionDescription = collections.namedtuple(\n",
    "    \"NPRegressionDescription\",\n",
    "    (\"query\", \"target_y\", \"num_total_points\", \"num_context_points\"))\n",
    "\n",
    "\n",
    "class GPCurvesReader(object):\n",
    "  \"\"\"Generates curves using a Gaussian Process (GP).\n",
    "\n",
    "  Supports vector inputs (x) and vector outputs (y). Kernel is\n",
    "  mean-squared exponential, using the x-value l2 coordinate distance scaled by\n",
    "  some factor chosen randomly in a range. Outputs are independent gaussian\n",
    "  processes.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               batch_size,\n",
    "               max_num_context,\n",
    "               x_size=1,\n",
    "               y_size=1,\n",
    "               l1_scale=0.6,\n",
    "               sigma_scale=1.0,\n",
    "               random_kernel_parameters=True,\n",
    "               testing=False):\n",
    "    \"\"\"Creates a regression dataset of functions sampled from a GP.\n",
    "\n",
    "    Args:\n",
    "      batch_size: An integer.\n",
    "      max_num_context: The max number of observations in the context.\n",
    "      x_size: Integer >= 1 for length of \"x values\" vector.\n",
    "      y_size: Integer >= 1 for length of \"y values\" vector.\n",
    "      l1_scale: Float; typical scale for kernel distance function.\n",
    "      sigma_scale: Float; typical scale for variance.\n",
    "      random_kernel_parameters: If `True`, the kernel parameters (l1 and sigma) \n",
    "          will be sampled uniformly within [0.1, l1_scale] and [0.1, sigma_scale].\n",
    "      testing: Boolean that indicates whether we are testing. If so there are\n",
    "          more targets for visualization.\n",
    "    \"\"\"\n",
    "    self._batch_size = batch_size\n",
    "    self._max_num_context = max_num_context\n",
    "    self._x_size = x_size\n",
    "    self._y_size = y_size\n",
    "    self._l1_scale = l1_scale\n",
    "    self._sigma_scale = sigma_scale\n",
    "    self._random_kernel_parameters = random_kernel_parameters\n",
    "    self._testing = testing\n",
    "\n",
    "  def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
    "    \"\"\"Applies the Gaussian kernel to generate curve data.\n",
    "\n",
    "    Args:\n",
    "      xdata: Tensor of shape [B, num_total_points, x_size] with\n",
    "          the values of the x-axis data.\n",
    "      l1: Tensor of shape [B, y_size, x_size], the scale\n",
    "          parameter of the Gaussian kernel.\n",
    "      sigma_f: Tensor of shape [B, y_size], the magnitude\n",
    "          of the std.\n",
    "      sigma_noise: Float, std of the noise that we add for stability.\n",
    "\n",
    "    Returns:\n",
    "      The kernel, a float tensor of shape\n",
    "      [B, y_size, num_total_points, num_total_points].\n",
    "    \"\"\"\n",
    "    num_total_points = tf.shape(xdata)[1]\n",
    "\n",
    "    # Expand and take the difference\n",
    "    xdata1 = tf.expand_dims(xdata, axis=1)  # [B, 1, num_total_points, x_size]\n",
    "    xdata2 = tf.expand_dims(xdata, axis=2)  # [B, num_total_points, 1, x_size]\n",
    "    diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n",
    "\n",
    "    # [B, y_size, num_total_points, num_total_points, x_size]\n",
    "    norm = tf.square(diff[:, None, :, :, :] / l1[:, :, None, None, :])\n",
    "\n",
    "    norm = tf.reduce_sum(\n",
    "        norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
    "\n",
    "    # [B, y_size, num_total_points, num_total_points]\n",
    "    kernel = tf.square(sigma_f)[:, :, None, None] * tf.exp(-0.5 * norm)\n",
    "\n",
    "    # Add some noise to the diagonal to make the cholesky work.\n",
    "    kernel += (sigma_noise**2) * tf.eye(num_total_points)\n",
    "\n",
    "    return kernel\n",
    "\n",
    "  def generate_curves(self):\n",
    "    \"\"\"Builds the op delivering the data.\n",
    "\n",
    "    Generated functions are `float32` with x values between -2 and 2.\n",
    "    \n",
    "    Returns:\n",
    "      A `NPRegressionDescription` namedtuple.\n",
    "    \"\"\"\n",
    "    num_context = tf.random.uniform(\n",
    "        shape=[], minval=3, maxval=self._max_num_context, dtype=tf.int32)\n",
    "\n",
    "    # If we are testing we want to have more targets and have them evenly\n",
    "    # distributed in order to plot the function.\n",
    "    if self._testing:\n",
    "      num_target = 400\n",
    "      num_total_points = num_target\n",
    "      x_values = tf.tile(\n",
    "          tf.expand_dims(tf.range(-2., 2., 1. / 100, dtype=tf.float32), axis=0),\n",
    "          [self._batch_size, 1])\n",
    "      x_values = tf.expand_dims(x_values, axis=-1)\n",
    "    # During training the number of target points and their x-positions are\n",
    "    # selected at random\n",
    "    else:\n",
    "      num_target = tf.random.uniform(shape=(), minval=0, \n",
    "                                     maxval=self._max_num_context - num_context,\n",
    "                                     dtype=tf.int32)\n",
    "      num_total_points = num_context + num_target\n",
    "      x_values = tf.random.uniform(\n",
    "          [self._batch_size, num_total_points, self._x_size], -2, 2)\n",
    "\n",
    "    # Set kernel parameters\n",
    "    # Either choose a set of random parameters for the mini-batch\n",
    "    if self._random_kernel_parameters:\n",
    "      l1 = tf.random.uniform([self._batch_size, self._y_size,\n",
    "                              self._x_size], 0.1, self._l1_scale)\n",
    "      sigma_f = tf.random.uniform([self._batch_size, self._y_size],\n",
    "                                  0.1, self._sigma_scale)\n",
    "    # Or use the same fixed parameters for all mini-batches\n",
    "    else:\n",
    "      l1 = tf.ones(shape=[self._batch_size, self._y_size,\n",
    "                          self._x_size]) * self._l1_scale\n",
    "      sigma_f = tf.ones(shape=[self._batch_size,\n",
    "                               self._y_size]) * self._sigma_scale\n",
    "\n",
    "    # Pass the x_values through the Gaussian kernel\n",
    "    # [batch_size, y_size, num_total_points, num_total_points]\n",
    "    kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
    "\n",
    "    # Calculate Cholesky, using double precision for better stability:\n",
    "    cholesky = tf.cast(tf.linalg.cholesky(tf.cast(kernel, tf.float64)), tf.float32)\n",
    "\n",
    "    # Sample a curve\n",
    "    # [batch_size, y_size, num_total_points, 1]\n",
    "    y_values = tf.matmul(\n",
    "        cholesky,\n",
    "        tf.random.normal([self._batch_size, self._y_size, num_total_points, 1]))\n",
    "\n",
    "    # [batch_size, num_total_points, y_size]\n",
    "    y_values = tf.transpose(tf.squeeze(y_values, 3), [0, 2, 1])\n",
    "\n",
    "    if self._testing:\n",
    "      # Select the targets\n",
    "      target_x = x_values\n",
    "      target_y = y_values\n",
    "\n",
    "      # Select the observations\n",
    "      idx = tf.random.shuffle(tf.range(num_target))\n",
    "      context_x = tf.gather(x_values, idx[:num_context], axis=1)\n",
    "      context_y = tf.gather(y_values, idx[:num_context], axis=1)\n",
    "\n",
    "    else:\n",
    "      # Select the targets which will consist of the context points as well as\n",
    "      # some new target points\n",
    "      target_x = x_values[:, :num_target + num_context, :]\n",
    "      target_y = y_values[:, :num_target + num_context, :]\n",
    "\n",
    "      # Select the observations\n",
    "      context_x = x_values[:, :num_context, :]\n",
    "      context_y = y_values[:, :num_context, :]\n",
    "#       print(context_x)\n",
    "#       print(context_y.shape)\n",
    "#       print(target_x.shape)\n",
    "    query = ((context_x, context_y), target_x)\n",
    "#     print(query.shape)\n",
    "    return NPRegressionDescription(\n",
    "        query=query,\n",
    "        target_y=target_y,\n",
    "        num_total_points=tf.shape(target_x)[1],\n",
    "        num_context_points=num_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-panel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "floppy-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = 100000 #@param {type:\"number\"}\n",
    "MAX_CONTEXT_POINTS = 50 #@param {type:\"number\"}\n",
    "PLOT_AFTER = 10000 #@param {type:\"number\"}\n",
    "HIDDEN_SIZE = 128 #@param {type:\"number\"}\n",
    "MODEL_TYPE = 'NP' #@param ['NP','ANP']\n",
    "ATTENTION_TYPE = 'uniform' #@param ['uniform','laplace','dot_product','multihead']\n",
    "random_kernel_parameters=True #@param {type:\"boolean\"}\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "# Train dataset\n",
    "dataset_train = GPCurvesReader(\n",
    "    batch_size=16, max_num_context=MAX_CONTEXT_POINTS, random_kernel_parameters=random_kernel_parameters)\n",
    "data_train = dataset_train.generate_curves()\n",
    "\n",
    "# Test dataset\n",
    "dataset_test = GPCurvesReader(\n",
    "    batch_size=1, max_num_context=MAX_CONTEXT_POINTS, testing=True, random_kernel_parameters=random_kernel_parameters)\n",
    "data_test = dataset_test.generate_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "outer-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_x = np.array(data_train.query[0][0])\n",
    "c_y = np.array(data_train.query[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "removed-assets",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 39, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aging-restriction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 39, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dimensional-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x = np.array(data_test.query[0][0])\n",
    "t_y = np.array(data_test.query[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "directed-mobile",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "educational-shelter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "overall-mexican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 39, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.from_numpy(c_x)\n",
    "y = torch.from_numpy(c_y)\n",
    "z = torch.cat([x, y], dim= -1)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-negotiation",
   "metadata": {},
   "source": [
    "## Data loading in torch.Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sacred-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NutrientsDataset(Dataset):\n",
    "    def __init__(self, df, num_context=40, num_extra_target=10):\n",
    "        self.df = df\n",
    "        self.num_context = num_context\n",
    "        self.num_extra_target = num_extra_target\n",
    "\n",
    "    def get_rows(self, i):\n",
    "        rows = self.df.iloc[i : i + (self.num_context + self.num_extra_target)].copy()\n",
    "        x = rows.iloc[:,:2].copy()\n",
    "        y = rows.iloc[:,2:].copy()\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.get_rows(i)\n",
    "        return x.values, y.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df) - (self.num_context + self.num_extra_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "associate-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npsample_batch(x, y, size=None, sort=False):\n",
    "    \n",
    "    \"\"\"Sample from numpy arrays along 2nd dim.\"\"\"\n",
    "    inds = np.random.choice(range(x.shape[1]), size=size, replace=False)\n",
    "    return x[:, inds], y[:, inds]\n",
    "\n",
    "def collate_fns(max_num_context, max_num_extra_target, sample, sort=True, context_in_target=True):\n",
    "    def collate_fn(batch, sample=sample):\n",
    "        # Collate\n",
    "        x = np.stack([x for x, y in batch], 0)\n",
    "        y = np.stack([y for x, y in batch], 0)\n",
    "\n",
    "        # Sample a subset of random size\n",
    "        num_context = np.random.randint(4, max_num_context)\n",
    "        num_extra_target = np.random.randint(4, max_num_extra_target)\n",
    "\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "\n",
    "        \n",
    "        x_context = x[:, :max_num_context]\n",
    "        y_context = y[:, :max_num_context]\n",
    "    \n",
    "        x_target_extra = x[:, max_num_context:]\n",
    "        y_target_extra = y[:, max_num_context:]\n",
    "        \n",
    "        if sample:\n",
    "\n",
    "            x_context, y_context = npsample_batch(\n",
    "                x_context, y_context, size=num_context\n",
    "            )\n",
    "\n",
    "            x_target_extra, y_target_extra = npsample_batch(\n",
    "                x_target_extra, y_target_extra, size=num_extra_target, sort=sort\n",
    "            )\n",
    "\n",
    "        # do we want to compute loss over context+target_extra, or focus in on only target_extra?\n",
    "        if context_in_target:\n",
    "            x_target = torch.cat([x_context, x_target_extra], 1)\n",
    "            y_target = torch.cat([y_context, y_target_extra], 1)\n",
    "        else:\n",
    "            x_target = x_target_extra\n",
    "            y_target = y_target_extra\n",
    "\n",
    "        \n",
    "        return x_context, y_context, x_target, y_target\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "spatial-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparamas = dict(num_context = 15,\n",
    "               num_extra_target = 16,\n",
    "               batch_size = 40,\n",
    "               context_in_target = False)\n",
    "train_df = NutrientsDataset(data_train,hparamas['num_context'],hparamas['num_extra_target'])\n",
    "\n",
    "train_loader = DataLoader(data_train,\n",
    "                          batch_size=hparamas['batch_size'],\n",
    "                         shuffle = True,\n",
    "                         collate_fn=collate_fns(\n",
    "                             hparamas['num_context'],hparamas['num_extra_target'], True,hparamas['context_in_target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-sampling",
   "metadata": {},
   "source": [
    " ## NP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "tutorial-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseNPBlock(nn.Module):\n",
    "    \"\"\"relu non-linearities for NP block\"\"\"\n",
    "    def __init__(self, inp_size,op_size, norm, bias = False, p = 0):\n",
    "        \"\"\"init function for linear2d class\n",
    "        \n",
    "        parameters\n",
    "        ----------\n",
    "        inp_size : int\n",
    "                input dimension for the Encoder part (d_in)\n",
    "        op_size : int\n",
    "                output dimension for Encoder part(d_out)\n",
    "        norm : str\n",
    "                normalization to be applied on linear output\n",
    "                pass norm == 'batch' to apply batch normalization\n",
    "                else dropout normalization is applied\n",
    "        bias : bool\n",
    "                if True, bias is included for linear layer else discarded\n",
    "        p : float\n",
    "                probality to be considered while applying Dropout regularization\n",
    "                \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm = norm\n",
    "        self.linear = nn.Linear(inp_size,op_size,bias = bias)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.batch_norm = nn.BatchNorm2d(op_size)\n",
    "        self.dropout = nn.Dropout2d(p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        x = self.batch_norm(x.permute(0,2,1)[:,:,:,None]) if self.norm == 'batch' else self.dropout(x.permute(0,2,1)[:,:,:,None])\n",
    "        \n",
    "        x = self.relu(x[:,:,:,0].permute(0,2,1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "complicated-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_MLP(nn.Module):\n",
    "    \"\"\" Batch MLP layer for NP-Encoder\"\"\"\n",
    "    def __init__(self, in_size, op_size, num_layers, norm, p = 0):\n",
    "        \"\"\"init function for linear2d class\n",
    "        \n",
    "        parameters\n",
    "        ----------\n",
    "        inp_size : int\n",
    "                input dimension for the Encoder part (d_in)\n",
    "        op_size : int\n",
    "                output dimension for Encoder part(d_out)\n",
    "        norm : str\n",
    "                normalization to be applied on linear output\n",
    "                pass norm == 'batch' to apply batch normalization\n",
    "                else dropout normalization is applied\n",
    "                \n",
    "        return torch.tensor of size (B,num_context_points,d_out)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.op_size = op_size\n",
    "        self.num_layers = num_layers\n",
    "        self.norm  = norm\n",
    "        \n",
    "        self.first_layer = baseNPBlock(in_size, op_size, self.norm, False,p)\n",
    "        self.encoder = nn.Sequential(*[batch_MLP(op_size, op_size, self.norm, False, p) for layer in range(self.num_layers-2)])\n",
    "        self.last_layer = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.first_layer(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.last_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "anticipated-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self,in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.linear = nn.linear(in_ch, out_ch, bias = False)\n",
    "        torch.nn.init.normal_(self.linear.weight,std = in_ch**0.5) #initilize weight matrix\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    \n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim, \n",
    "        attn_type, \n",
    "        attn_layers,\n",
    "        x_dim, \n",
    "        rep='mlp',\n",
    "        n_multiheads = 8,\n",
    "        norm = 'dropout',\n",
    "        p = 0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self._rep = rep\n",
    "        \n",
    "        if self._rep =='mlp':\n",
    "            \n",
    "            #Both Key and Value needs to have same dimension\n",
    "            self.batch_mlpk = batch_MLP(x_dim, hidden_dim, attn_layers, norm )\n",
    "            self.batch_mlpq = batch_MLP(x_dim, hidden_dim, attn_layers, norm, )\n",
    "        \n",
    "        \n",
    "        if attn_type == 'uniform':\n",
    "            self.attn_func = self.uniform_attn\n",
    "        if attn_type=='laplace':\n",
    "            self.attn_func = self.laplace_attn\n",
    "        if attn_type == 'dot':\n",
    "            self.attn_func = self.dot_attn\n",
    "        elif attn_type == 'multihead':\n",
    "            self.W_k = nn.ModuleList([LinearAttention(hidden_dim,hidden_dim) for head in range(n_multiheads)])\n",
    "            self.W_v = nn.ModuleList([LinearAttention(hidden_dim,hidden_dim) for head in range(n_multiheads)])\n",
    "            self.W_q = nn.ModuleList([LinearAttention(hidden_dim,hidden_dim) for head in range(n_multiheads)])\n",
    "            \n",
    "            self.w = LinearAttention(hidden_dim*n_multiheads,hidden_dim)\n",
    "            self.attn_func = self.multihead_attn\n",
    "            self.num_heads = n_multiheads\n",
    "            \n",
    "            \n",
    "            \n",
    "    def forward(self, k, q, v):\n",
    "        if self.rep =='mlp':\n",
    "            k = self.batch_mlpk(k)\n",
    "            q = self.batch_mlpq(q)\n",
    "        \n",
    "        rep = self.attn_func(k,q,v)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    \n",
    "    def uniform_attn(self,k,q,v):\n",
    "        num_points = q.shape[1]\n",
    "        rep = torch.mean(v, axis = 1, keepdim = True)\n",
    "        rep = rep.repeat(1,num_points,1)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    def laplace_attn(self, k, q, v, scale = 0.5):\n",
    "        k = k.unsqueeze(1)\n",
    "        v = v.unsqueeze(2)\n",
    "        \n",
    "        w = torch.abs((k-v)*scale)\n",
    "        w = w.sum(dim = -1)\n",
    "        weight = torch.softmax(w, dim = -1)\n",
    "        \n",
    "        #batch matrix multiplication (einstein summation convention for tenso)\n",
    "        rep = torch.einsum(\"bik, bkj -> bij\",weight, v)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    \n",
    "    def dot_product_attn(self, q, k, v, normalize):\n",
    "        \n",
    "        β = q.shape[-1]**0.5\n",
    "        w_unnorm = torch.einsum('bjk,bik->bij', k, q)/β\n",
    "        weight = torch.softmax(w_unnorm, dim = -1)\n",
    "        rep = torch.einsum(\"bik, bkj -> bij\",weight, v)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    def multihead_attn(self, k , q, v):\n",
    "        outs = []\n",
    "        \n",
    "        for i in range(sel.n_multiheads):\n",
    "            k = self.w_k(k)\n",
    "            q = self.w_q(q)\n",
    "            v = self.w_v(v)\n",
    "            \n",
    "            out = self.dot_product_attn(k,q,v)\n",
    "            outs.append(out)\n",
    "            \n",
    "        outs = torch.stack(outs, dim = -1)\n",
    "        outs = out.views(outs.shape[0], outs.shape[-1], -1)\n",
    "        rep = self.w(outs)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "important-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttentionModule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "contained-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicEncoder(nn.Module):\n",
    "    def __init__(\n",
    "                self,\n",
    "                in_dim,\n",
    "                key_dim,\n",
    "                norm,\n",
    "                hidden_dim = 32,\n",
    "                encoder_layer = 2,\n",
    "                self_attn_type ='dot',\n",
    "                cross_attn_type ='dot',\n",
    "                p_encoder = 0,\n",
    "                p_attention = 0,\n",
    "                attn_layers = 2,\n",
    "                use_self_attn = False\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._use_attn = use_self_attn\n",
    "        \n",
    "        self.encoder = batch_MLP(in_dim, hidden_dim, encoder_layer,norm, p_encoder)\n",
    "        \n",
    "        if self._use_attn:\n",
    "            self.self_attn = AttentionModule(hidden_dim, self_attn_type, attn_layers,x_dim, rep = 'mlp',norm = norm, p = p_attention)\n",
    "            \n",
    "        self.cross_attn = AttentionModule(hidden_dim, cross_attn_type, attn_layers, x_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, context_x, context_y, target_x):\n",
    "        #concatenate context_x, context_y along the last dim.\n",
    "        det_enc_in = torch.cat([context_x, context_y], dim = -1)\n",
    "        \n",
    "        det_encoded = self.encoder(det_enc_in)\n",
    "        \n",
    "        if self.use_self_attn:\n",
    "            det_encoded = self.self_attn(det_encoded, det_encoded, det_encoded)\n",
    "            \n",
    "        h = self.cross_attn(context_x, det_encoded, target_x)\n",
    "        \n",
    "        return h\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "unlikely-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "class latentEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_dim,\n",
    "                norm,\n",
    "                hidden_dim = 32,\n",
    "                latent_dim = 32,\n",
    "                self_attn_type = 'dot',\n",
    "                encoder_layer = 3,\n",
    "                min_std = 0.01,\n",
    "                p_encoder = 0,\n",
    "                p_attn = 0,\n",
    "                use_self_attn = False,\n",
    "                attn_layers = 2,\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self._use_attn = use_self_attn\n",
    "        \n",
    "        self.encoder = batch_MLP(in_dim, hidden_dim, encoder_layer,norm, p_encoder)\n",
    "        \n",
    "        if self._use_attn:\n",
    "            self.self_attn = AttentionModule(hidden_dim, self_attn_type, attn_layers,x_dim, rep = 'identity',norm = norm, p = p_attention)\n",
    "        \n",
    "        self.secondlast_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.l_sigma = nn.Linear(hidden_dim, hidden_dim) \n",
    "        self.min_std = min_std\n",
    "        self.use_lvar = use_lvar\n",
    "        self.use_attn = use_self_attn\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x,y):\n",
    "        encoder_inp = torch.cat([x,y], dim = -1) #(B, n, hd)\n",
    "        \n",
    "        encoded_op = self.encoder(encoder_inp)#(B, n, hd)\n",
    "        \n",
    "        if self.use_attn:\n",
    "            encoded_op = self.self_attn(encoded_op, encoded_op, encoded_op) #(B, n, hd)\n",
    "            \n",
    "        \n",
    "        mean_val = torch.mean(encoded_op, dim = 1) #mean aggregation (B, 1, hd)\n",
    "        \n",
    "        #further MLP layer that maps parameters to gaussian latent\n",
    "        mean_repr = torch.relu(self.secondlast_layer(mean_val)) #(B, 1, hd)\n",
    "        \n",
    "        μ = self.mean(mean_repr)\n",
    "        \n",
    "        log_sigma = self.l_sigma(mean_repr)\n",
    "        \n",
    "        #to avoid mode collapse\n",
    "        σ = min_std + (1-min_std)*F.logsigmoid(log_sigma)\n",
    "        \n",
    "        dist = torch.distributions.Normal(μ, σ)\n",
    "        \n",
    "        return dist\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 x_dim,\n",
    "                 y_dim,\n",
    "                 hidden_dim = 32,\n",
    "                 latent_dim = 32,\n",
    "                 n_decoder_layer = 3,\n",
    "                 use_deterministic_path = True,\n",
    "                 min_std = 0.01,\n",
    "                 norm = 'dropout',\n",
    "                 dropout_p = 0,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = norm\n",
    "        self.target_transform = nn.Linear(x_dim, hidden_dim)\n",
    "        \n",
    "        if use_deterministic_path:\n",
    "            hidden_dim_2 = 2 * hidden_dim + latent_dim\n",
    "        else:\n",
    "            hidden_dim_2 = hidden_dim + latent_dim\n",
    "            \n",
    "        self.decoder = batch_MLP(hidden_dim_2, hidden_dim_2)\n",
    "        \n",
    "        self.mean = nn.Linear(hidden_dim_2, y_dim)\n",
    "        self.std = nn.Linear(hidden_dim_2, y_dim)\n",
    "        self.deterministic_path = use_deterministic_path\n",
    "        self.min_std = min_std\n",
    "        \n",
    "        \n",
    "    def forward(self, t_x, r, z):\n",
    "        x = self.target_transform(x)\n",
    "        \n",
    "        if self.deterministic_path:\n",
    "            z = torch.cat([r,z], dim = -1)\n",
    "            \n",
    "        r = torch.cat([z,x], dim = -1)\n",
    "        \n",
    "        r = self.decoder(r)\n",
    "        \n",
    "        mean = self.mean(r)\n",
    "        log_sigma = self.std(r)\n",
    "        \n",
    "        #clamp sigmad\n",
    "        sigma = self._min_std + (1 - self._min_std) * F.softplus(log_sigma)\n",
    "        \n",
    "        dist = torch.distributions.Normal(mean,sigma)\n",
    "        \n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentModel(nn.Module):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
