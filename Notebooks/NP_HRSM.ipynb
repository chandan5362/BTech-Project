{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "graduate-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install utm\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "equipped-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import utm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "official-growing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>OC</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.401111</td>\n",
       "      <td>17.894722</td>\n",
       "      <td>1.08</td>\n",
       "      <td>756.0</td>\n",
       "      <td>9.43</td>\n",
       "      <td>834.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73.401389</td>\n",
       "      <td>17.894722</td>\n",
       "      <td>1.12</td>\n",
       "      <td>781.2</td>\n",
       "      <td>9.21</td>\n",
       "      <td>265.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73.402222</td>\n",
       "      <td>17.894722</td>\n",
       "      <td>0.68</td>\n",
       "      <td>478.8</td>\n",
       "      <td>8.99</td>\n",
       "      <td>318.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.403056</td>\n",
       "      <td>17.894722</td>\n",
       "      <td>1.76</td>\n",
       "      <td>1234.8</td>\n",
       "      <td>9.65</td>\n",
       "      <td>954.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.403333</td>\n",
       "      <td>17.894722</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1247.4</td>\n",
       "      <td>8.77</td>\n",
       "      <td>371.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lon        lat    OC       N     P       K\n",
       "0  73.401111  17.894722  1.08   756.0  9.43  834.37\n",
       "1  73.401389  17.894722  1.12   781.2  9.21  265.10\n",
       "2  73.402222  17.894722  0.68   478.8  8.99  318.96\n",
       "3  73.403056  17.894722  1.76  1234.8  9.65  954.77\n",
       "4  73.403333  17.894722  1.78  1247.4  8.77  371.77"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"../Dataset/Maharashtra_Soil_Nutrients_Data.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-lancaster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incorporate-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_coord(x,y):\n",
    "    \"\"\"\n",
    "    parameters\n",
    "    ----------\n",
    "    x : numpy array, float64\n",
    "        list of longitude cordinates\n",
    "    y : numpy array, float64\n",
    "        list of latitude cordinates\n",
    "        \n",
    "    return\n",
    "    ------\n",
    "    scaled(0-1) x and y\n",
    "    \"\"\"\n",
    "    x = (x-x.min())/(x.max()-x.min())\n",
    "    y = (y-y.min())/(y.max()-y.min())\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "contained-friday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "val_col = ['OC','N','P','K']\n",
    "values = data[val_col]\n",
    "coordinates = data[['lon','lat']]\n",
    "#lat,lon to utm projection\n",
    "\n",
    "x,y,zone,ut = utm.from_latlon(coordinates['lat'].values,coordinates['lon'].values)\n",
    "\n",
    "lon,lat = y/1000,x/1000 #in km\n",
    "\n",
    "# lon, lat = scaled_coord(lon,lat)\n",
    "#normalize values of OC, N, K, P\n",
    "\n",
    "#standardise lon and lat\n",
    "lon = (lon-np.mean(lon))/np.std(lon)\n",
    "lat = (lat-np.mean(lat))/np.std(lat)\n",
    "\n",
    "test_k = MinMaxScaler().fit_transform(values)\n",
    "values = test_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "about-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lon'] = lon\n",
    "data['lat'] = lat\n",
    "for i,col in enumerate(val_col):\n",
    "    data[col] = values[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "headed-twist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>OC</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.425536</td>\n",
       "      <td>-2.402803</td>\n",
       "      <td>0.011632</td>\n",
       "      <td>0.079661</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.012262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.425540</td>\n",
       "      <td>-2.402303</td>\n",
       "      <td>0.012067</td>\n",
       "      <td>0.082316</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.003893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.425550</td>\n",
       "      <td>-2.400805</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.050450</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.004684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.425561</td>\n",
       "      <td>-2.399307</td>\n",
       "      <td>0.019024</td>\n",
       "      <td>0.130115</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.014032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.425565</td>\n",
       "      <td>-2.398808</td>\n",
       "      <td>0.019241</td>\n",
       "      <td>0.131443</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.005461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lon       lat        OC         N         P         K\n",
       "0 -1.425536 -2.402803  0.011632  0.079661  0.001163  0.012262\n",
       "1 -1.425540 -2.402303  0.012067  0.082316  0.001136  0.003893\n",
       "2 -1.425550 -2.400805  0.007283  0.050450  0.001109  0.004684\n",
       "3 -1.425561 -2.399307  0.019024  0.130115  0.001190  0.014032\n",
       "4 -1.425565 -2.398808  0.019241  0.131443  0.001082  0.005461"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "billion-armor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split dataset into train and test\n",
    "# split the dataset into train and test dataset\n",
    "ix = np.random.choice(data.shape[0],int(data.shape[0]*0.2),replace = False)\n",
    "data_train = data.iloc[[int(i) for i in range(data.shape[0]) if i not in ix]].reset_index(drop = True)\n",
    "data_test = data.iloc[ix].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "assisted-burns",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20837, 6), (5209, 6))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "revolutionary-milwaukee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>OC</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.425550</td>\n",
       "      <td>-2.400805</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.050450</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.004684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.425561</td>\n",
       "      <td>-2.399307</td>\n",
       "      <td>0.019024</td>\n",
       "      <td>0.130115</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.014032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.425565</td>\n",
       "      <td>-2.398808</td>\n",
       "      <td>0.019241</td>\n",
       "      <td>0.131443</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.005461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.425575</td>\n",
       "      <td>-2.397310</td>\n",
       "      <td>0.016306</td>\n",
       "      <td>0.111527</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.007821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.425579</td>\n",
       "      <td>-2.396811</td>\n",
       "      <td>0.019241</td>\n",
       "      <td>0.131443</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.005461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lon       lat        OC         N         P         K\n",
       "0 -1.425550 -2.400805  0.007283  0.050450  0.001109  0.004684\n",
       "1 -1.425561 -2.399307  0.019024  0.130115  0.001190  0.014032\n",
       "2 -1.425565 -2.398808  0.019241  0.131443  0.001082  0.005461\n",
       "3 -1.425575 -2.397310  0.016306  0.111527  0.001136  0.007821\n",
       "4 -1.425579 -2.396811  0.019241  0.131443  0.001082  0.005461"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "minute-consultation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>OC</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.320061</td>\n",
       "      <td>0.954405</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.006153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.368445</td>\n",
       "      <td>0.777145</td>\n",
       "      <td>0.008370</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.001833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.277226</td>\n",
       "      <td>-1.176953</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.063433</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>0.010698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.953466</td>\n",
       "      <td>0.131978</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.016392</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.005824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.182705</td>\n",
       "      <td>0.294285</td>\n",
       "      <td>0.007066</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.006582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lon       lat        OC         N         P         K\n",
       "0  0.320061  0.954405  0.005109  0.012552  0.001154  0.006153\n",
       "1 -0.368445  0.777145  0.008370  0.011376  0.000635  0.001833\n",
       "2  2.277226 -1.176953  0.000326  0.063433  0.003633  0.010698\n",
       "3 -0.953466  0.131978  0.005979  0.016392  0.000775  0.005824\n",
       "4 -1.182705  0.294285  0.007066  0.048680  0.001227  0.006582"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-shepherd",
   "metadata": {},
   "source": [
    "## GP Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vital-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "small-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The (A)NP takes as input a `NPRegressionDescription` namedtuple with fields:\n",
    "#   `query`: a tuple containing ((context_x, context_y), target_x)\n",
    "#   `target_y`: a tensor containing the ground truth for the targets to be\n",
    "#     predicted\n",
    "#   `num_total_points`: A vector containing a scalar that describes the total\n",
    "#     number of datapoints used (context + target)\n",
    "#   `num_context_points`: A vector containing a scalar that describes the number\n",
    "#     of datapoints used as context\n",
    "# The GPCurvesReader returns the newly sampled data in this format at each\n",
    "# iteration\n",
    "\n",
    "NPRegressionDescription = collections.namedtuple(\n",
    "    \"NPRegressionDescription\",\n",
    "    (\"query\", \"target_y\", \"num_total_points\", \"num_context_points\"))\n",
    "\n",
    "\n",
    "class GPCurvesReader(object):\n",
    "  \"\"\"Generates curves using a Gaussian Process (GP).\n",
    "\n",
    "  Supports vector inputs (x) and vector outputs (y). Kernel is\n",
    "  mean-squared exponential, using the x-value l2 coordinate distance scaled by\n",
    "  some factor chosen randomly in a range. Outputs are independent gaussian\n",
    "  processes.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               batch_size,\n",
    "               max_num_context,\n",
    "               x_size=1,\n",
    "               y_size=1,\n",
    "               l1_scale=0.6,\n",
    "               sigma_scale=1.0,\n",
    "               random_kernel_parameters=True,\n",
    "               testing=False):\n",
    "    \"\"\"Creates a regression dataset of functions sampled from a GP.\n",
    "\n",
    "    Args:\n",
    "      batch_size: An integer.\n",
    "      max_num_context: The max number of observations in the context.\n",
    "      x_size: Integer >= 1 for length of \"x values\" vector.\n",
    "      y_size: Integer >= 1 for length of \"y values\" vector.\n",
    "      l1_scale: Float; typical scale for kernel distance function.\n",
    "      sigma_scale: Float; typical scale for variance.\n",
    "      random_kernel_parameters: If `True`, the kernel parameters (l1 and sigma) \n",
    "          will be sampled uniformly within [0.1, l1_scale] and [0.1, sigma_scale].\n",
    "      testing: Boolean that indicates whether we are testing. If so there are\n",
    "          more targets for visualization.\n",
    "    \"\"\"\n",
    "    self._batch_size = batch_size\n",
    "    self._max_num_context = max_num_context\n",
    "    self._x_size = x_size\n",
    "    self._y_size = y_size\n",
    "    self._l1_scale = l1_scale\n",
    "    self._sigma_scale = sigma_scale\n",
    "    self._random_kernel_parameters = random_kernel_parameters\n",
    "    self._testing = testing\n",
    "\n",
    "  def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
    "    \"\"\"Applies the Gaussian kernel to generate curve data.\n",
    "\n",
    "    Args:\n",
    "      xdata: Tensor of shape [B, num_total_points, x_size] with\n",
    "          the values of the x-axis data.\n",
    "      l1: Tensor of shape [B, y_size, x_size], the scale\n",
    "          parameter of the Gaussian kernel.\n",
    "      sigma_f: Tensor of shape [B, y_size], the magnitude\n",
    "          of the std.\n",
    "      sigma_noise: Float, std of the noise that we add for stability.\n",
    "\n",
    "    Returns:\n",
    "      The kernel, a float tensor of shape\n",
    "      [B, y_size, num_total_points, num_total_points].\n",
    "    \"\"\"\n",
    "    num_total_points = tf.shape(xdata)[1]\n",
    "\n",
    "    # Expand and take the difference\n",
    "    xdata1 = tf.expand_dims(xdata, axis=1)  # [B, 1, num_total_points, x_size]\n",
    "    xdata2 = tf.expand_dims(xdata, axis=2)  # [B, num_total_points, 1, x_size]\n",
    "    diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n",
    "\n",
    "    # [B, y_size, num_total_points, num_total_points, x_size]\n",
    "    norm = tf.square(diff[:, None, :, :, :] / l1[:, :, None, None, :])\n",
    "\n",
    "    norm = tf.reduce_sum(\n",
    "        norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
    "\n",
    "    # [B, y_size, num_total_points, num_total_points]\n",
    "    kernel = tf.square(sigma_f)[:, :, None, None] * tf.exp(-0.5 * norm)\n",
    "\n",
    "    # Add some noise to the diagonal to make the cholesky work.\n",
    "    kernel += (sigma_noise**2) * tf.eye(num_total_points)\n",
    "\n",
    "    return kernel\n",
    "\n",
    "  def generate_curves(self):\n",
    "    \"\"\"Builds the op delivering the data.\n",
    "\n",
    "    Generated functions are `float32` with x values between -2 and 2.\n",
    "    \n",
    "    Returns:\n",
    "      A `NPRegressionDescription` namedtuple.\n",
    "    \"\"\"\n",
    "    num_context = tf.random.uniform(\n",
    "        shape=[], minval=3, maxval=self._max_num_context, dtype=tf.int32)\n",
    "\n",
    "    # If we are testing we want to have more targets and have them evenly\n",
    "    # distributed in order to plot the function.\n",
    "    if self._testing:\n",
    "      num_target = 400\n",
    "      num_total_points = num_target\n",
    "      x_values = tf.tile(\n",
    "          tf.expand_dims(tf.range(-2., 2., 1. / 100, dtype=tf.float32), axis=0),\n",
    "          [self._batch_size, 1])\n",
    "      x_values = tf.expand_dims(x_values, axis=-1)\n",
    "    # During training the number of target points and their x-positions are\n",
    "    # selected at random\n",
    "    else:\n",
    "      num_target = tf.random.uniform(shape=(), minval=0, \n",
    "                                     maxval=self._max_num_context - num_context,\n",
    "                                     dtype=tf.int32)\n",
    "      num_total_points = num_context + num_target\n",
    "      x_values = tf.random.uniform(\n",
    "          [self._batch_size, num_total_points, self._x_size], -2, 2)\n",
    "\n",
    "    # Set kernel parameters\n",
    "    # Either choose a set of random parameters for the mini-batch\n",
    "    if self._random_kernel_parameters:\n",
    "      l1 = tf.random.uniform([self._batch_size, self._y_size,\n",
    "                              self._x_size], 0.1, self._l1_scale)\n",
    "      sigma_f = tf.random.uniform([self._batch_size, self._y_size],\n",
    "                                  0.1, self._sigma_scale)\n",
    "    # Or use the same fixed parameters for all mini-batches\n",
    "    else:\n",
    "      l1 = tf.ones(shape=[self._batch_size, self._y_size,\n",
    "                          self._x_size]) * self._l1_scale\n",
    "      sigma_f = tf.ones(shape=[self._batch_size,\n",
    "                               self._y_size]) * self._sigma_scale\n",
    "\n",
    "    # Pass the x_values through the Gaussian kernel\n",
    "    # [batch_size, y_size, num_total_points, num_total_points]\n",
    "    kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
    "\n",
    "    # Calculate Cholesky, using double precision for better stability:\n",
    "    cholesky = tf.cast(tf.linalg.cholesky(tf.cast(kernel, tf.float64)), tf.float32)\n",
    "\n",
    "    # Sample a curve\n",
    "    # [batch_size, y_size, num_total_points, 1]\n",
    "    y_values = tf.matmul(\n",
    "        cholesky,\n",
    "        tf.random.normal([self._batch_size, self._y_size, num_total_points, 1]))\n",
    "\n",
    "    # [batch_size, num_total_points, y_size]\n",
    "    y_values = tf.transpose(tf.squeeze(y_values, 3), [0, 2, 1])\n",
    "\n",
    "    if self._testing:\n",
    "      # Select the targets\n",
    "      target_x = x_values\n",
    "      target_y = y_values\n",
    "\n",
    "      # Select the observations\n",
    "      idx = tf.random.shuffle(tf.range(num_target))\n",
    "      context_x = tf.gather(x_values, idx[:num_context], axis=1)\n",
    "      context_y = tf.gather(y_values, idx[:num_context], axis=1)\n",
    "\n",
    "    else:\n",
    "      # Select the targets which will consist of the context points as well as\n",
    "      # some new target points\n",
    "      target_x = x_values[:, :num_target + num_context, :]\n",
    "      target_y = y_values[:, :num_target + num_context, :]\n",
    "\n",
    "      # Select the observations\n",
    "      context_x = x_values[:, :num_context, :]\n",
    "      context_y = y_values[:, :num_context, :]\n",
    "#       print(context_x)\n",
    "#       print(context_y.shape)\n",
    "#       print(target_x.shape)\n",
    "    query = ((context_x, context_y), target_x)\n",
    "#     print(query.shape)\n",
    "    return NPRegressionDescription(\n",
    "        query=query,\n",
    "        target_y=target_y,\n",
    "        num_total_points=tf.shape(target_x)[1],\n",
    "        num_context_points=num_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-panel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "floppy-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = 100000 #@param {type:\"number\"}\n",
    "MAX_CONTEXT_POINTS = 50 #@param {type:\"number\"}\n",
    "PLOT_AFTER = 10000 #@param {type:\"number\"}\n",
    "HIDDEN_SIZE = 128 #@param {type:\"number\"}\n",
    "MODEL_TYPE = 'NP' #@param ['NP','ANP']\n",
    "ATTENTION_TYPE = 'uniform' #@param ['uniform','laplace','dot_product','multihead']\n",
    "random_kernel_parameters=True #@param {type:\"boolean\"}\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "# Train dataset\n",
    "dataset_train = GPCurvesReader(\n",
    "    batch_size=16, max_num_context=MAX_CONTEXT_POINTS, random_kernel_parameters=random_kernel_parameters)\n",
    "data_train = dataset_train.generate_curves()\n",
    "\n",
    "# Test dataset\n",
    "dataset_test = GPCurvesReader(\n",
    "    batch_size=1, max_num_context=MAX_CONTEXT_POINTS, testing=True, random_kernel_parameters=random_kernel_parameters)\n",
    "data_test = dataset_test.generate_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "outer-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data generator\n",
    "(c_x,c_y),t_x = data_train.query\n",
    "t_y = data_train.target_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "removed-assets",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 11, 1)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_x = c_x.numpy()\n",
    "c_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aging-restriction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 11, 1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_y = c_y.numpy()\n",
    "c_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dimensional-latter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 21, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x = t_x.numpy()\n",
    "t_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "directed-mobile",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 21, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_y = t_y.numpy()\n",
    "t_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "educational-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "overall-mexican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 11, 2])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.from_numpy(c_x)\n",
    "y = torch.from_numpy(c_y)\n",
    "z = torch.cat([x, y], dim= -1)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-negotiation",
   "metadata": {},
   "source": [
    "## Data loading in torch.Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "sacred-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NutrientsDataset(Dataset):\n",
    "    def __init__(self, df, num_context=40, num_extra_target=10):\n",
    "        self.df = df\n",
    "        self.num_context = num_context\n",
    "        self.num_extra_target = num_extra_target\n",
    "\n",
    "    def get_rows(self, i):\n",
    "        rows = self.df.iloc[i : i + (self.num_context + self.num_extra_target)].copy()\n",
    "        x = rows.iloc[:,:2].copy()\n",
    "        y = rows.iloc[:,2:].copy()\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.get_rows(i)\n",
    "        return x.values, y.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df) - (self.num_context + self.num_extra_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "associate-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npsample_batch(x, y, size=None, sort=False):\n",
    "    \n",
    "    \"\"\"Sample from numpy arrays along 2nd dim.\"\"\"\n",
    "    inds = np.random.choice(range(x.shape[1]), size=size, replace=False)\n",
    "    return x[:, inds], y[:, inds]\n",
    "\n",
    "def collate_fns(max_num_context, max_num_extra_target, sample, sort=True, context_in_target=True):\n",
    "    def collate_fn(batch, sample=sample):\n",
    "        # Collate\n",
    "        x = np.stack([x for x, y in batch], 0)\n",
    "        y = np.stack([y for x, y in batch], 0)\n",
    "\n",
    "        # Sample a subset of random size\n",
    "        num_context = np.random.randint(4, max_num_context)\n",
    "        num_extra_target = np.random.randint(4, max_num_extra_target)\n",
    "\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "\n",
    "        \n",
    "        x_context = x[:, :max_num_context]\n",
    "        y_context = y[:, :max_num_context]\n",
    "    \n",
    "        x_target_extra = x[:, max_num_context:]\n",
    "        y_target_extra = y[:, max_num_context:]\n",
    "        \n",
    "        if sample:\n",
    "\n",
    "            x_context, y_context = npsample_batch(\n",
    "                x_context, y_context, size=num_context\n",
    "            )\n",
    "\n",
    "            x_target_extra, y_target_extra = npsample_batch(\n",
    "                x_target_extra, y_target_extra, size=num_extra_target, sort=sort\n",
    "            )\n",
    "\n",
    "        # do we want to compute loss over context+target_extra, or focus in on only target_extra?\n",
    "        if context_in_target:\n",
    "            x_target = torch.cat([x_context, x_target_extra], 1)\n",
    "            y_target = torch.cat([y_context, y_target_extra], 1)\n",
    "        else:\n",
    "            x_target = x_target_extra\n",
    "            y_target = y_target_extra\n",
    "\n",
    "        \n",
    "        return x_context, y_context, x_target, y_target\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "spatial-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparamas = dict(num_context = 15,\n",
    "               num_extra_target = 16,\n",
    "               batch_size = 40,\n",
    "               context_in_target = False)\n",
    "train_df = NutrientsDataset(data_train,hparamas['num_context'],hparamas['num_extra_target'])\n",
    "\n",
    "train_loader = DataLoader(data_train,\n",
    "                          batch_size=hparamas['batch_size'],\n",
    "                         shuffle = True,\n",
    "                         collate_fn=collate_fns(\n",
    "                             hparamas['num_context'],hparamas['num_extra_target'], True,hparamas['context_in_target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-sampling",
   "metadata": {},
   "source": [
    " ## NP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "tutorial-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseNPBlock(nn.Module):\n",
    "    \"\"\"relu non-linearities for NP block\"\"\"\n",
    "    def __init__(self, inp_size,op_size, norm, bias = False, p = 0):\n",
    "        \"\"\"init function for linear2d class\n",
    "        \n",
    "        parameters\n",
    "        ----------\n",
    "        inp_size : int\n",
    "                input dimension for the Encoder part (d_in)\n",
    "        op_size : int\n",
    "                output dimension for Encoder part(d_out)\n",
    "        norm : str\n",
    "                normalization to be applied on linear output\n",
    "                pass norm == 'batch' to apply batch normalization\n",
    "                else dropout normalization is applied\n",
    "        bias : bool\n",
    "                if True, bias is included for linear layer else discarded\n",
    "        p : float\n",
    "                probality to be considered while applying Dropout regularization\n",
    "                \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm = norm\n",
    "        self.linear = nn.Linear(inp_size,op_size,bias = bias)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.batch_norm = nn.BatchNorm2d(op_size)\n",
    "        self.dropout = nn.Dropout2d(p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        x = self.batch_norm(x.permute(0,2,1)[:,:,:,None]) if self.norm == 'batch' else self.dropout(x.permute(0,2,1)[:,:,:,None])\n",
    "        \n",
    "        x = self.relu(x[:,:,:,0].permute(0,2,1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "complicated-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_MLP(nn.Module):\n",
    "    \"\"\" Batch MLP layer for NP-Encoder\"\"\"\n",
    "    def __init__(self, in_size, op_size, num_layers, norm, p = 0):\n",
    "        \"\"\"init function for linear2d class\n",
    "        \n",
    "        parameters\n",
    "        ----------\n",
    "        inp_size : int\n",
    "                input dimension for the Encoder part (d_in)\n",
    "        op_size : int\n",
    "                output dimension for Encoder part(d_out)\n",
    "        norm : str\n",
    "                normalization to be applied on linear output\n",
    "                pass norm == 'batch' to apply batch normalization\n",
    "                else dropout normalization is applied\n",
    "                \n",
    "        return torch.tensor of size (B,num_context_points,d_out)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.op_size = op_size\n",
    "        self.num_layers = num_layers\n",
    "        self.norm  = norm\n",
    "        \n",
    "        self.first_layer = baseNPBlock(in_size, op_size, self.norm, False,p)\n",
    "        self.encoder = nn.Sequential(*[batch_MLP(op_size, op_size, self.norm, False, p) for layer in range(self.num_layers-2)])\n",
    "        self.last_layer = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.first_layer(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.last_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "anticipated-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self,in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_ch, out_ch, bias = False)\n",
    "        torch.nn.init.normal_(self.linear.weight,std = in_ch**0.5) #initilize weight matrix\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    \n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim, \n",
    "        attn_type , \n",
    "        attn_layers,\n",
    "        x_dim, \n",
    "        rep='mlp',\n",
    "        n_multiheads = 8,\n",
    "        norm = 'dropout',\n",
    "        p = 0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.rep = rep\n",
    "#         self.n_multiheads = n_multiheads\n",
    "        # rep determines whether raw input given to the model would be used as key and query or\n",
    "        # it's output through MLP. \n",
    "        if self.rep =='mlp':\n",
    "            \n",
    "            #Both Key and Value needs to have same dimension\n",
    "            self.batch_mlpk = batch_MLP(x_dim, hidden_dim, attn_layers, norm ,p)\n",
    "            self.batch_mlpq = batch_MLP(x_dim, hidden_dim, attn_layers, norm, p)\n",
    "        \n",
    "        \n",
    "        if attn_type == 'uniform':\n",
    "            self.attn_func = self.uniform_attn\n",
    "        if attn_type=='laplace':\n",
    "            self.attn_func = self.laplace_attn\n",
    "        if attn_type == 'dot':\n",
    "            self.attn_func = self.dot_attn\n",
    "        elif attn_type == 'multihead':\n",
    "            self.w_k = nn.ModuleList([LinearAttention(hidden_dim,hidden_dim) for head in range(n_multiheads)])\n",
    "            self.w_v = nn.ModuleList([LinearAttention(hidden_dim,hidden_dim) for head in range(n_multiheads)])\n",
    "            self.w_q = nn.ModuleList([LinearAttention(hidden_dim,hidden_dim) for head in range(n_multiheads)])\n",
    "            \n",
    "            self.w = LinearAttention(hidden_dim*n_multiheads,hidden_dim)\n",
    "            self.attn_func = self.multihead_attn\n",
    "            self.num_heads = n_multiheads\n",
    "            \n",
    "            \n",
    "            \n",
    "    def forward(self, k, q, v):\n",
    "        if self.rep =='mlp':\n",
    "            k = self.batch_mlpk(k) #(B, n, H)\n",
    "            q = self.batch_mlpq(q) #(B, m, H)\n",
    "        \n",
    "        rep = self.attn_func(k,q,v)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    \n",
    "    def uniform_attn(self, k, q, v):\n",
    "        num_points = q.shape[1]\n",
    "        rep = torch.mean(v, axis = 1, keepdim = True)\n",
    "        rep = rep.repeat(1,num_points,1)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    def laplace_attn(self, k, q, v, scale = 0.5):\n",
    "        k = k.unsqueeze(1)\n",
    "        v = v.unsqueeze(2)\n",
    "        \n",
    "        w = torch.abs((k-v)*scale)\n",
    "        w = w.sum(dim = -1)\n",
    "        weight = torch.softmax(w, dim = -1)\n",
    "        \n",
    "        #batch matrix multiplication (einstein summation convention for tensor)\n",
    "        rep = torch.einsum(\"bik, bkj -> bij\",weight, v)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    \n",
    "    def dot_product_attn(self, k, q, v):\n",
    "#         print(\"k =\",k.shape)\n",
    "#         print(\"q =\",q.shape)\n",
    "#         print(\"v =\",v.shape)    \n",
    "        β = q.shape[-1]**0.5\n",
    "        w_unnorm = torch.einsum('bjk,bik->bij', k, q)/β\n",
    "#         print(\"w_unnorm =\",w_unnorm.shape)\n",
    "        \n",
    "        weight = torch.softmax(w_unnorm, dim = -1)\n",
    "        rep = torch.einsum(\"bik, bkj -> bij\",weight, v)\n",
    "#         print(\"rep =\",rep.shape)\n",
    "        return rep\n",
    "    \n",
    "    def multihead_attn(self, k , q, v):\n",
    "        outs = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            k = self.w_k[i](k) #(B, n, H)\n",
    "#             print(\"k =\",k.shape)\n",
    "            q = self.w_q[i](q) #(B, m, H)\n",
    "#             print(\"q =\",q.shape)\n",
    "            v = self.w_v[i](v) #(B, n, H)\n",
    "#             print(\"v =\",v.shape)\n",
    "            out = self.dot_product_attn(k, q, v)\n",
    "            outs.append(out)\n",
    "            \n",
    "        outs = torch.stack(outs, dim = -1) #(B, m, H, n_heads)\n",
    "#         print(\"outs dim =\", outs.shape)\n",
    "        outs = outs.view(outs.shape[0], outs.shape[1], -1) #(B, m, n_heads*H)\n",
    "#         print(\"outs shape =\",outs.shape)\n",
    "        rep = self.w(outs) #(B, m, H)\n",
    "        \n",
    "        return rep\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "important-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttentionModule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "contained-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicEncoder(nn.Module):\n",
    "    def __init__(\n",
    "                self,\n",
    "                in_dim,\n",
    "                x_dim,\n",
    "                norm = 'dropout',\n",
    "                hidden_dim = 32,\n",
    "                encoder_layer = 2,\n",
    "                self_attn_type ='dot',\n",
    "                cross_attn_type ='dot',\n",
    "                p_encoder = 0,\n",
    "                p_attention = 0,\n",
    "                attn_layers = 2,\n",
    "                use_self_attn = False\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_self_attn = use_self_attn\n",
    "        \n",
    "        self.encoder = batch_MLP(in_dim, hidden_dim, encoder_layer,norm, p_encoder)\n",
    "        \n",
    "        if self.use_self_attn:\n",
    "            self.self_attn = AttentionModule(hidden_dim, self_attn_type, attn_layers,x_dim, rep = 'mlp',norm = norm, p = p_attention)\n",
    "            \n",
    "        self.cross_attn = AttentionModule(hidden_dim, cross_attn_type, attn_layers, x_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, context_x, context_y, target_x):\n",
    "        #concatenate context_x, context_y along the last dim.\n",
    "        det_enc_in = torch.cat([context_x, context_y], dim = -1)\n",
    "        \n",
    "        det_encoded = self.encoder(det_enc_in) #(B, n, hd)\n",
    "        \n",
    "        if self.use_self_attn:\n",
    "            det_encoded = self.self_attn(det_encoded, det_encoded, det_encoded)\n",
    "            \n",
    "        h = self.cross_attn(context_x, target_x, det_encoded)\n",
    "        \n",
    "        return h\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "unlikely-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_dim,\n",
    "                hidden_dim = 32,\n",
    "                latent_dim = 32,\n",
    "                self_attn_type = 'dot',\n",
    "                encoder_layer = 3,\n",
    "                min_std = 0.01,\n",
    "                norm = 'dropout',\n",
    "                p_encoder = 0,\n",
    "                p_attn = 0,\n",
    "                use_self_attn = False,\n",
    "                attn_layers = 2,\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self._use_attn = use_self_attn\n",
    "        \n",
    "        self.encoder = batch_MLP(in_dim, hidden_dim, encoder_layer,norm, p_encoder)\n",
    "        \n",
    "        if self._use_attn:\n",
    "            self.self_attn = AttentionModule(hidden_dim, self_attn_type, attn_layers,x_dim, rep = 'identity',norm = norm, p = p_attention)\n",
    "        \n",
    "        self.secondlast_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.l_sigma = nn.Linear(hidden_dim, hidden_dim) \n",
    "        self.min_std = min_std\n",
    "#         self.use_lvar = use_lvar\n",
    "        self.use_attn = use_self_attn\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x,y):\n",
    "        encoder_inp = torch.cat([x,y], dim = -1) \n",
    "        \n",
    "        encoded_op = self.encoder(encoder_inp)#(B, n, hd)\n",
    "#         print(\"encoder_op shape = \",encoded_op.shape)\n",
    "        if self.use_attn:\n",
    "            encoded_op = self.self_attn(encoded_op, encoded_op, encoded_op) #(B, n, hd)\n",
    "            \n",
    "        \n",
    "        mean_val = torch.mean(encoded_op, dim = 1) #mean aggregation (B, hd)\n",
    "        \n",
    "        #further MLP layer that maps parameters to gaussian latent\n",
    "        mean_repr = torch.relu(self.secondlast_layer(mean_val)) #(B, hd)\n",
    "        \n",
    "        μ = self.mean(mean_repr) # (B, ld)\n",
    "#         print(\"mean = \", μ.shape)\n",
    "        log_scale = self.l_sigma(mean_repr) #(B, ld)\n",
    "        \n",
    "        #to avoid mode collapse\n",
    "        σ = self.min_std + (1-self.min_std)*torch.sigmoid(log_scale*0.5) #(b, ld)\n",
    "#         print(σ)\n",
    "        dist = torch.distributions.Normal(μ, σ)\n",
    "        \n",
    "        return dist\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fluid-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 x_dim,\n",
    "                 y_dim,\n",
    "                 hidden_dim = 32,\n",
    "                 latent_dim = 32,\n",
    "                 n_decoder_layer = 3,\n",
    "                 use_deterministic_path = True,\n",
    "                 min_std = 0.01,\n",
    "                 norm = 'dropout',\n",
    "                 dropout_p = 0,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = norm\n",
    "        self.target_transform = nn.Linear(x_dim, hidden_dim)\n",
    "        \n",
    "        if use_deterministic_path:\n",
    "            hidden_dim_2 = 2 * hidden_dim + latent_dim\n",
    "        else:\n",
    "            hidden_dim_2 = hidden_dim + latent_dim\n",
    "            \n",
    "        self.decoder = batch_MLP(hidden_dim_2, hidden_dim_2, n_decoder_layer, norm, dropout_p)\n",
    "        \n",
    "        self.mean = nn.Linear(hidden_dim_2, y_dim)\n",
    "        self.std = nn.Linear(hidden_dim_2, y_dim)\n",
    "        self.deterministic_path = use_deterministic_path\n",
    "        self.min_std = min_std\n",
    "        \n",
    "        \n",
    "    def forward(self, r, z, t_x):\n",
    "        x = self.target_transform(t_x)\n",
    "        \n",
    "        if self.deterministic_path:\n",
    "            z = torch.cat([r,z], dim = -1)\n",
    "#             print(\"z.shape =\", z.shape)\n",
    "        r = torch.cat([z,x], dim = -1)\n",
    "        \n",
    "        r = self.decoder(r)\n",
    "        \n",
    "        mean = self.mean(r)\n",
    "        log_sigma = self.std(r)\n",
    "        \n",
    "        #clamp sigmad\n",
    "        sigma = self.min_std + (1 - self.min_std) * F.softplus(log_sigma)\n",
    "        \n",
    "        dist = torch.distributions.Normal(mean,sigma)\n",
    "        \n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-zealand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "artificial-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentModel(nn.Module):\n",
    "    def __init__(self,\n",
    "               x_dim,\n",
    "               y_dim,\n",
    "               hidden_dim = 32,\n",
    "               latent_dim = 32,\n",
    "               latent_self_attn_type = 'multihead',\n",
    "                det_self_attn_type = 'multihead',\n",
    "                det_cross_attn_type = 'multihead',\n",
    "               n_lat_enc_layer = 2,\n",
    "               n_det_enc_layer = 2,\n",
    "               n_decoder_layer = 2,\n",
    "               use_deterministic_enc = False,\n",
    "               min_std = 0.01,\n",
    "               p_drop = 0,\n",
    "               norm = 'dropout',\n",
    "               p_attn_drop = 0,\n",
    "               attn_layers = 2,\n",
    "               use_self_attn = False,\n",
    "               context_in_target = True,\n",
    "                training = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.laten_encoder = LatentEncoder(x_dim+y_dim,\n",
    "                                           hidden_dim=hidden_dim,\n",
    "                                           latent_dim=latent_dim,\n",
    "                                           self_attn_type=latent_self_attn_type,\n",
    "                                           encoder_layer=n_lat_enc_layer,\n",
    "                                           min_std=min_std,\n",
    "                                           norm = norm,\n",
    "                                           p_encoder=p_drop,\n",
    "                                           p_attn=p_attn_drop,\n",
    "                                           use_self_attn=use_self_attn,\n",
    "                                           attn_layers=attn_layers \n",
    "                                          )\n",
    "        self.deterministic_encoder = DeterministicEncoder(x_dim+y_dim,\n",
    "                                                          x_dim,\n",
    "                                                          norm = norm,\n",
    "                                                          hidden_dim=hidden_dim,\n",
    "                                                          encoder_layer=n_det_enc_layer,\n",
    "                                                          self_attn_type=det_self_attn_type,\n",
    "                                                          cross_attn_type=det_cross_attn_type,\n",
    "                                                          p_encoder=p_drop,\n",
    "                                                          p_attention=p_attn_drop,\n",
    "                                                          attn_layers=attn_layers,\n",
    "                                                          use_self_attn=use_self_attn\n",
    "                                                         )\n",
    "        self.decoder = Decoder(x_dim,\n",
    "                              y_dim,\n",
    "                              hidden_dim  = hidden_dim,\n",
    "                              latent_dim=latent_dim,\n",
    "                              n_decoder_layer=n_decoder_layer,\n",
    "                              use_deterministic_path=use_deterministic_enc,\n",
    "                              min_std=min_std,\n",
    "                              norm=norm,\n",
    "                              dropout_p=p_drop\n",
    "                              )\n",
    "        self.use_deterministic_enc = use_deterministic_enc\n",
    "        self.context_in_target = context_in_target\n",
    "        self.training = training\n",
    "        \n",
    "        \n",
    "    def forward(self, c_x, c_y, t_x, t_y = None):\n",
    "        dist_prior = self.laten_encoder(c_x, c_y)\n",
    "\n",
    "        if t_y is not None:\n",
    "            dist_posterior = self.laten_encoder(t_x, t_y)\n",
    "            z = dist_posterior.loc\n",
    "        else:\n",
    "            z = dist_prior.loc\n",
    "            \n",
    "        n_target = t_x.shape[1]\n",
    "        z = z.unsqueeze(1).repeat(1, n_target,1) #(B, n_target, L)\n",
    "        \n",
    "        if self.use_deterministic_enc:\n",
    "            r = self.deterministic_encoder(c_x, c_y, t_x) #(B, n_target=m, H)\n",
    "#             print(r.shape)\n",
    "        else:\n",
    "            r = None\n",
    "            \n",
    "        dist = self.decoder(r, z, t_x)\n",
    "        \n",
    "        #at test time, target y is not Known so we return None\n",
    "        if t_y is not None:\n",
    "            log_p = dist.log_prob(t_y).mean(-1)\n",
    "            kl_loss = torch.distributions.kl_divergence(dist_posterior, dist_prior).mean(-1)\n",
    "            kl_loss = kl_loss[:,None].expand(log_p.shape)\n",
    "            loss = (kl_loss-log_p).mean()\n",
    "            mse_loss = F.mse_loss(dist.loc, t_y, reduction = 'none')[:,:c_x.size(1)].mean()\n",
    "        else:\n",
    "            kl_loss  =None\n",
    "            log_p = None\n",
    "            mse_loss = None\n",
    "            loss = None\n",
    "            \n",
    "        y_pred = dist.rsample() if self.training else dist.loc\n",
    "            \n",
    "        return y_pred,  dict(loss = loss, loss_p = log_p, loss_kl = kl_loss, loss_mse = mse_loss), dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-documentation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "proved-mainland",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cx = torch.from_numpy(c_x)\n",
    "cy = torch.from_numpy(c_y)\n",
    "tx = torch.from_numpy(t_x)\n",
    "ty = torch.from_numpy(t_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "former-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "As = torch.randn(3,2,5)\n",
    "Bs = torch.randn(3,5,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-joseph",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-married",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-comfort",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-familiar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-dakota",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-maple",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-interpretation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
