{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reported-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "going-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The (A)NP takes as input a `NPRegressionDescription` namedtuple with fields:\n",
    "#   `query`: a tuple containing ((context_x, context_y), target_x)\n",
    "#   `target_y`: a tensor containing the ground truth for the targets to be\n",
    "#     predicted\n",
    "#   `num_total_points`: A vector containing a scalar that describes the total\n",
    "#     number of datapoints used (context + target)\n",
    "#   `num_context_points`: A vector containing a scalar that describes the number\n",
    "#     of datapoints used as context\n",
    "# The GPCurvesReader returns the newly sampled data in this format at each\n",
    "# iteration\n",
    "\n",
    "NPRegressionDescription = collections.namedtuple(\n",
    "    \"NPRegressionDescription\",\n",
    "    (\"query\", \"target_y\", \"num_total_points\", \"num_context_points\"))\n",
    "\n",
    "\n",
    "class GPCurvesReader(object):\n",
    "  \"\"\"Generates curves using a Gaussian Process (GP).\n",
    "\n",
    "  Supports vector inputs (x) and vector outputs (y). Kernel is\n",
    "  mean-squared exponential, using the x-value l2 coordinate distance scaled by\n",
    "  some factor chosen randomly in a range. Outputs are independent gaussian\n",
    "  processes.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               batch_size,\n",
    "               max_num_context,\n",
    "               x_size=1,\n",
    "               y_size=1,\n",
    "               l1_scale=0.6,\n",
    "               sigma_scale=1.0,\n",
    "               random_kernel_parameters=True,\n",
    "               testing=False):\n",
    "    \"\"\"Creates a regression dataset of functions sampled from a GP.\n",
    "\n",
    "    Args:\n",
    "      batch_size: An integer.\n",
    "      max_num_context: The max number of observations in the context.\n",
    "      x_size: Integer >= 1 for length of \"x values\" vector.\n",
    "      y_size: Integer >= 1 for length of \"y values\" vector.\n",
    "      l1_scale: Float; typical scale for kernel distance function.\n",
    "      sigma_scale: Float; typical scale for variance.\n",
    "      random_kernel_parameters: If `True`, the kernel parameters (l1 and sigma) \n",
    "          will be sampled uniformly within [0.1, l1_scale] and [0.1, sigma_scale].\n",
    "      testing: Boolean that indicates whether we are testing. If so there are\n",
    "          more targets for visualization.\n",
    "    \"\"\"\n",
    "    self._batch_size = batch_size\n",
    "    self._max_num_context = max_num_context\n",
    "    self._x_size = x_size\n",
    "    self._y_size = y_size\n",
    "    self._l1_scale = l1_scale\n",
    "    self._sigma_scale = sigma_scale\n",
    "    self._random_kernel_parameters = random_kernel_parameters\n",
    "    self._testing = testing\n",
    "\n",
    "  def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
    "    \"\"\"Applies the Gaussian kernel to generate curve data.\n",
    "\n",
    "    Args:\n",
    "      xdata: Tensor of shape [B, num_total_points, x_size] with\n",
    "          the values of the x-axis data.\n",
    "      l1: Tensor of shape [B, y_size, x_size], the scale\n",
    "          parameter of the Gaussian kernel.\n",
    "      sigma_f: Tensor of shape [B, y_size], the magnitude\n",
    "          of the std.\n",
    "      sigma_noise: Float, std of the noise that we add for stability.\n",
    "\n",
    "    Returns:\n",
    "      The kernel, a float tensor of shape\n",
    "      [B, y_size, num_total_points, num_total_points].\n",
    "    \"\"\"\n",
    "    num_total_points = tf.shape(xdata)[1]\n",
    "\n",
    "    # Expand and take the difference\n",
    "    xdata1 = tf.expand_dims(xdata, axis=1)  # [B, 1, num_total_points, x_size]\n",
    "    xdata2 = tf.expand_dims(xdata, axis=2)  # [B, num_total_points, 1, x_size]\n",
    "    diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n",
    "\n",
    "    # [B, y_size, num_total_points, num_total_points, x_size]\n",
    "    norm = tf.square(diff[:, None, :, :, :] / l1[:, :, None, None, :])\n",
    "\n",
    "    norm = tf.reduce_sum(\n",
    "        norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
    "\n",
    "    # [B, y_size, num_total_points, num_total_points]\n",
    "    kernel = tf.square(sigma_f)[:, :, None, None] * tf.exp(-0.5 * norm)\n",
    "\n",
    "    # Add some noise to the diagonal to make the cholesky work.\n",
    "    kernel += (sigma_noise**2) * tf.eye(num_total_points)\n",
    "\n",
    "    return kernel\n",
    "\n",
    "  def generate_curves(self):\n",
    "    \"\"\"Builds the op delivering the data.\n",
    "\n",
    "    Generated functions are `float32` with x values between -2 and 2.\n",
    "    \n",
    "    Returns:\n",
    "      A `NPRegressionDescription` namedtuple.\n",
    "    \"\"\"\n",
    "    num_context = tf.random.uniform(\n",
    "        shape=[], minval=3, maxval=self._max_num_context, dtype=tf.int32)\n",
    "\n",
    "    # If we are testing we want to have more targets and have them evenly\n",
    "    # distributed in order to plot the function.\n",
    "    if self._testing:\n",
    "      num_target = 400\n",
    "      num_total_points = num_target\n",
    "      x_values = tf.tile(\n",
    "          tf.expand_dims(tf.range(-2., 2., 1. / 100, dtype=tf.float32), axis=0),\n",
    "          [self._batch_size, 1])\n",
    "      x_values = tf.expand_dims(x_values, axis=-1)\n",
    "    # During training the number of target points and their x-positions are\n",
    "    # selected at random\n",
    "    else:\n",
    "      num_target = tf.random.uniform(shape=(), minval=0, \n",
    "                                     maxval=self._max_num_context - num_context,\n",
    "                                     dtype=tf.int32)\n",
    "      num_total_points = num_context + num_target\n",
    "      x_values = tf.random.uniform(\n",
    "          [self._batch_size, num_total_points, self._x_size], -2, 2)\n",
    "\n",
    "    # Set kernel parameters\n",
    "    # Either choose a set of random parameters for the mini-batch\n",
    "    if self._random_kernel_parameters:\n",
    "      l1 = tf.random.uniform([self._batch_size, self._y_size,\n",
    "                              self._x_size], 0.1, self._l1_scale)\n",
    "      sigma_f = tf.random.uniform([self._batch_size, self._y_size],\n",
    "                                  0.1, self._sigma_scale)\n",
    "    # Or use the same fixed parameters for all mini-batches\n",
    "    else:\n",
    "      l1 = tf.ones(shape=[self._batch_size, self._y_size,\n",
    "                          self._x_size]) * self._l1_scale\n",
    "      sigma_f = tf.ones(shape=[self._batch_size,\n",
    "                               self._y_size]) * self._sigma_scale\n",
    "\n",
    "    # Pass the x_values through the Gaussian kernel\n",
    "    # [batch_size, y_size, num_total_points, num_total_points]\n",
    "    kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
    "\n",
    "    # Calculate Cholesky, using double precision for better stability:\n",
    "    cholesky = tf.cast(tf.linalg.cholesky(tf.cast(kernel, tf.float64)), tf.float32)\n",
    "\n",
    "    # Sample a curve\n",
    "    # [batch_size, y_size, num_total_points, 1]\n",
    "    y_values = tf.matmul(\n",
    "        cholesky,\n",
    "        tf.random.normal([self._batch_size, self._y_size, num_total_points, 1]))\n",
    "\n",
    "    # [batch_size, num_total_points, y_size]\n",
    "    y_values = tf.transpose(tf.squeeze(y_values, 3), [0, 2, 1])\n",
    "\n",
    "    if self._testing:\n",
    "      # Select the targets\n",
    "      target_x = x_values\n",
    "      target_y = y_values\n",
    "\n",
    "      # Select the observations\n",
    "      idx = tf.random.shuffle(tf.range(num_target))\n",
    "      context_x = tf.gather(x_values, idx[:num_context], axis=1)\n",
    "      context_y = tf.gather(y_values, idx[:num_context], axis=1)\n",
    "\n",
    "    else:\n",
    "      # Select the targets which will consist of the context points as well as\n",
    "      # some new target points\n",
    "      target_x = x_values[:, :num_target + num_context, :]\n",
    "      target_y = y_values[:, :num_target + num_context, :]\n",
    "\n",
    "      # Select the observations\n",
    "      context_x = x_values[:, :num_context, :]\n",
    "      context_y = y_values[:, :num_context, :]\n",
    "#       print(context_x)\n",
    "#       print(context_y.shape)\n",
    "#       print(target_x.shape)\n",
    "    query = ((context_x, context_y), target_x)\n",
    "#     print(query.shape)\n",
    "    return NPRegressionDescription(\n",
    "        query=query,\n",
    "        target_y=target_y,\n",
    "        num_total_points=tf.shape(target_x)[1],\n",
    "        num_context_points=num_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quick-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = 100000 #@param {type:\"number\"}\n",
    "MAX_CONTEXT_POINTS = 50 #@param {type:\"number\"}\n",
    "PLOT_AFTER = 10000 #@param {type:\"number\"}\n",
    "HIDDEN_SIZE = 128 #@param {type:\"number\"}\n",
    "MODEL_TYPE = 'NP' #@param ['NP','ANP']\n",
    "ATTENTION_TYPE = 'uniform' #@param ['uniform','laplace','dot_product','multihead']\n",
    "random_kernel_parameters=True #@param {type:\"boolean\"}\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "# Train dataset\n",
    "dataset_train = GPCurvesReader(\n",
    "    batch_size=16, max_num_context=MAX_CONTEXT_POINTS, random_kernel_parameters=random_kernel_parameters)\n",
    "data_train = dataset_train.generate_curves()\n",
    "\n",
    "# Test dataset\n",
    "dataset_test = GPCurvesReader(\n",
    "    batch_size=1, max_num_context=MAX_CONTEXT_POINTS, testing=True, random_kernel_parameters=random_kernel_parameters)\n",
    "data_test = dataset_test.generate_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "turkish-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data generator\n",
    "(c_x,c_y),t_x = data_train.query\n",
    "t_y = data_train.target_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "possible-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data generator\n",
    "(c_xt,c_yt),t_xt = data_train.query\n",
    "t_yt = data_train.target_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-conducting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
